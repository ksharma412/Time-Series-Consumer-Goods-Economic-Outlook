---
title: "Consumer Goods"
author: "Kratika Sharma, Drake Walker, Krishnasai Sharawant Adiraju"
date: "5/10/2022"
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

##  {.tabset .tabset-fade}

### 1.Packages

**Packages to be installed**

Following Packages are needed for the project-

```{r}
#including required packages
suppressWarnings(suppressMessages(library(tidyverse)))
suppressWarnings(suppressMessages(library(tidyr)))
suppressWarnings(suppressMessages(library(fpp3)))
suppressWarnings(suppressMessages(library(stargazer)))
suppressWarnings(suppressMessages(library(tsibble)))
suppressWarnings(suppressMessages(library(latex2exp)))
suppressWarnings(suppressMessages(library(magrittr)))
suppressWarnings(suppressMessages(library(stringr)))
suppressWarnings(suppressMessages(library(seasonal)))
suppressWarnings(suppressMessages(library(ggplot2)))
suppressWarnings(suppressMessages(library(ggfortify)))
suppressWarnings(suppressMessages(library(dplyr)))
suppressWarnings(suppressMessages(library(magrittr))) 
suppressWarnings(suppressMessages(library(tidyr))) 
suppressWarnings(suppressMessages(library(ggplot2))) 
suppressWarnings(suppressMessages(library(stats))) 
suppressWarnings(suppressMessages(library(factoextra)))
suppressWarnings(suppressMessages(library(knitr)))
suppressWarnings(suppressMessages(library(kableExtra)))
suppressWarnings(suppressMessages(library(purrr)))
suppressWarnings(suppressMessages(library(gridExtra)))
```


### 2.Data Prepration

**Loading Tables**

Here all the datasets are loaded.

**2.1.Data Source** 

The data is downloaded from https://fred.stlouisfed.org/

```{r, echo = TRUE, include = TRUE}
#reading data from excel
library(readxl)
#real personal consumption expenditure: durable goods
data_rpce_durable_goods <- read_excel("D:\\Sem 2 - Spring 2022\\ECON 825\\Project\\Consumer goods Data\\RPCE\\ND000346Q (1).xls" )

#industrial production: durable goods
data_industrial_prod_durable_goods <- read_excel("D:\\Sem 2 - Spring 2022\\ECON 825\\Project\\Consumer goods Data\\IP\\IPB51100N.xls")

#Producer Price index by commodity : durable goods
data_PPI_durable_goods <- read_excel("D:\\Sem 2 - Spring 2022\\ECON 825\\Project\\Consumer goods Data\\PPI\\WPSFD413112.xls")

#Consumer Price index by commodity : durable goods
data_CPI_durable_goods <- read_excel("D:\\Sem 2 - Spring 2022\\ECON 825\\Project\\Consumer goods Data\\CPI\\CUSR0000SAD.xls")


#GDP
data_GDP <- read_excel("D:\\Sem 2 - Spring 2022\\ECON 825\\Project\\GDP\\Quaterly-non seasonal adjusted\\ND000334Q.xls")

# Non-Durable
#real personal consumption expenditure: non durable goods
data_rpce_ndurable_goods <- read_excel("D:\\Sem 2 - Spring 2022\\ECON 825\\Project\\Consumer goods Data\\Non-Durable\\RPCE\\ND000348Q.xls" )

#industrial production: non durable goods
data_industrial_prod_ndurable_goods <- read_excel("D:\\Sem 2 - Spring 2022\\ECON 825\\Project\\Consumer goods Data\\Non-Durable\\IP\\IPB51200N.xls")

#Producer Price index by commodity : non durable goods
data_PPI_ndurable_goods <- read_excel("D:\\Sem 2 - Spring 2022\\ECON 825\\Project\\Consumer goods Data\\Non-Durable\\PPI\\WPSFD49508.xls")

#Consumer Price index by commodity : non durable goods
data_CPI_ndurable_goods <- read_excel("D:\\Sem 2 - Spring 2022\\ECON 825\\Project\\Consumer goods Data\\Non-Durable\\CPI\\CUSR0000SAN.xls")

#Services

#real personal consumtion
data_rpce_services<-read_excel("D:\\Sem 2 - Spring 2022\\ECON 825\\Project\\Consumer goods Data\\Services\\ND000350Q.xls")

#all employees services providing
data_ae_sp<-read.csv("D:\\Sem 2 - Spring 2022\\ECON 825\\Project\\Consumer goods Data\\Services\\SRVPRD.csv", header = TRUE) %>%
mutate(Month = yearmonth(DATE)) %>%as_tsibble(index=Month)

#net export
data_netexp<-read_excel("D:\\Sem 2 - Spring 2022\\ECON 825\\Project\\Consumer goods Data\\Services\\ND000374Q.xls") 

#Real disposable income
data_rdpi<-read_excel("D:\\Sem 2 - Spring 2022\\ECON 825\\Project\\Real disposable income\\Quaterly-seasonal adjusted\\DPIC96.xls") %>%
mutate(Quarter = yearquarter(observation_date)) %>%as_tsibble(index=Quarter)

```


**2.2.Modifying Data as per requirements**

* Downloaded excel files were converted as tsibble
* Real Personal consumption expenditure and Net Export is mutated to show Expenditure as % of GDP
* Index for PPI, CPI, and Industrial Production is mutated to be 1982 = 100


```{r, echo = TRUE, include = TRUE}

#converting personal consumption expenditure to a % of expenditure in GDP 
data_rpce_durable_goods_gdp <- inner_join(data_rpce_durable_goods,data_GDP, by = "observation_date") 

rpce_durable_goods_gdp <- data_rpce_durable_goods_gdp %>% mutate(Quarter = yearquarter(observation_date), Expenditure = (ND000346Q*100/ND000334Q))%>%
  select(Quarter,Expenditure) %>% as_tsibble(index = Quarter)

industrial_prod_durable_goods <- data_industrial_prod_durable_goods %>% mutate(Month = yearmonth(observation_date), Index = IPB51100N)%>% mutate(Index = Index + 61.6696) %>%
  select(Month,Index) %>% as_tsibble(index = Month) 

PPI_durable_goods <- data_PPI_durable_goods %>% mutate(Month = yearmonth(observation_date), Index = WPSFD413112)%>%
  select(Month,Index) %>% as_tsibble(index = Month)

CPI_durable_goods <- data_CPI_durable_goods %>% mutate(Month = yearmonth(observation_date), Index = CUSR0000SAD) %>% mutate(Index = Index + 3.7)%>%
  select(Month,Index) %>% as_tsibble(index = Month)

GDP <- data_GDP %>% mutate(Quarter = yearquarter(observation_date), GDP = ND000334Q)%>%
  select(Quarter,GDP) %>% as_tsibble(index = Quarter)

head(rpce_durable_goods_gdp)
head(industrial_prod_durable_goods)
head(PPI_durable_goods)
head(CPI_durable_goods)
head(data_rdpi)

#non durables
#converting personal consumption expenditure to a % of expenditure in GDP 
data_rpce_ndurable_goods_gdp <- inner_join(data_rpce_ndurable_goods,data_GDP, by = "observation_date") 

rpce_ndurable_goods_gdp <- data_rpce_ndurable_goods_gdp %>% mutate(Quarter = yearquarter(observation_date), Expenditure = (ND000348Q*100/ND000334Q))%>%
  select(Quarter,Expenditure) %>% as_tsibble(index = Quarter)

industrial_prod_ndurable_goods <- data_industrial_prod_ndurable_goods %>% mutate(Month = yearmonth(observation_date), Index = IPB51200N)%>% mutate(Index = Index + 24.0739) %>%
  select(Month,Index) %>% as_tsibble(index = Month) 

PPI_ndurable_goods <- data_PPI_ndurable_goods %>% mutate(Month = yearmonth(observation_date), Index = WPSFD49508)%>%
  select(Month,Index) %>% as_tsibble(index = Month)

CPI_ndurable_goods <- data_CPI_ndurable_goods %>% mutate(Month = yearmonth(observation_date), Index = CUSR0000SAN) %>% mutate(Index = Index + 1.4)%>%
  select(Month,Index) %>% as_tsibble(index = Month)

GDP <- data_GDP %>% mutate(Quarter = yearquarter(observation_date), GDP = ND000334Q)%>%
  select(Quarter,GDP) %>% as_tsibble(index = Quarter)

head(rpce_ndurable_goods_gdp)
head(industrial_prod_ndurable_goods)
head(PPI_ndurable_goods)
head(CPI_ndurable_goods)

#Services

data_rpce_services_gdp <- inner_join(data_rpce_services,data_GDP, by = "observation_date") 

data_rpce_services_gdp <- data_rpce_services_gdp %>% mutate(Quarter = yearquarter(observation_date), Expenditure = (ND000350Q*100/ND000334Q))%>%
  select(Quarter,Expenditure) %>% as_tsibble(index = Quarter)

data_netexp_gdp <- inner_join(data_netexp,data_GDP, by = "observation_date") 

data_netexp_gdp <- data_netexp_gdp %>% mutate(Quarter = yearquarter(observation_date), Expenditure = (ND000374Q*100/ND000334Q))%>%
  select(Quarter,Expenditure) %>% as_tsibble(index = Quarter)


head(data_rpce_services_gdp)
head(data_netexp_gdp)
head(data_ae_sp)
```


### 3.Durable Goods - Modelling

**3.1.Real Personal Consumption Expenditure: Durable Goods**

<br>

**Looking at time series**
```{r}
#plotting graphs to see our time series
g1<- rpce_durable_goods_gdp %>%
  autoplot(Expenditure) + ggtitle("Real personal consumption Expenditure: Durable Goods") + ylab("% of GDP")
g2<- rpce_durable_goods_gdp %>%
  gg_season(Expenditure) + ggtitle("Real personal consumption Expenditure: Durable Goods")+ylab("% of GDP")
g3<- rpce_durable_goods_gdp %>%
  gg_subseries(Expenditure) + ggtitle("Real personal consumption Expenditure: Durable Goods")+ylab("% of GDP")
grid.arrange(g1, g2,g3, nrow = 3)
```

<br>
**Observations for Real Personal Consumption Expenditure: Durable Goods**

* There is an upward trend in this series, with some exception periods. Like, 2008-2009 saw a drop in the expenditure on durable goods. Which is expected as that was the time of great recession period
* Up till 2020, every year has seen a slight rise in Q2 followed by a drop in Q3 and then every year Q4 seems to have seen a peak of expenditure, which is holiday season
* 2020 and 2021 seems to be following a different trend. 2020 has seen a slight drop in expenditure in Q1 but after that the personal consumption expenditure starts forming a major part of GDP and the series seems to be rising sharply. This is right after covid lockdown started and was clear to people that it is going to stay for long. So, as expected people started spending more in durable goods
* So, this series has strong seasonality and trend in it

```{r}
#Checking for different components in the series
rpce_durable_goods_gdp %>%
  model(
    STL(Expenditure)) %>%
  components() %>%
  autoplot()+
  labs(title = "STL decomposition of RPCE: Consumer goods")
```

* So, there is a strong seasonality in the series which is increasing over time

<br>

**Forecast: RPCE**

We will build two competitive models : ARIMA and ETS on training data and check the best performing model on test data set for every forecasting exercise we will do by looking at its accuracy

* **Using ARIMA Model**

  * Step 1: Transform the series, if needed
  * Step 2: Check for stationarity by looking at ACF and running KPSS test
  * Step 3: Find number of seasonal and normal differencing needed in the data
  * Step 4: Plot differenced time series and run KPSS test again, check its residuals and ACF       and PACF plot
  * Step 5: Guess ARIMA models 
  * Step 6: Divide data into test and train
  * Step 7: Run ARIMA models on train data and select the best model
  * Step 8: Check residuals of the model, run Ljung Box test
  * Step 9: Forecast

```{r}
#transform the data and stabilize as there is variation in the series which is increasing with time
#Box-Cox transformation
lambda1 <- rpce_durable_goods_gdp %>%
  features(Expenditure, features = guerrero) %>%
  pull(lambda_guerrero)

rpce_durable_goods_gdp %>%
  autoplot(box_cox(Expenditure, lambda1)) +
  labs(y = "", title = TeX(paste0("Transformed Real personal consumption expenditure as % of GDP for Durable Goods with $\\lambda$ = ",
         round(lambda1,2))))

#Checking for stationary
rpce_durable_goods_gdp %>% ACF(Expenditure) %>% autoplot()+ggtitle("ACF plot for Real Personal Consumption Expenditure: Durable goods")

#Running KPSS to statistically justify that data is stationary or not and find order of the differincing - Null hypothesis (Ho): Data is stationary - Alternate hypothesis (Halt): Data is non-stationary
rpce_durable_goods_gdp %>% features(box_cox(Expenditure, lambda1), unitroot_kpss)

```

* The time series is non-stationary as it clearly has trend and seasonality in it, which we observed in above graphs
* The ACF plot is decaying very slowly, which is indicative of non-stationarity in the time series
* Also, the p-value <0.05 in KPSS test, so we reject null hypothesis of KPSS test and can confirm that data is not stationary 
* So, we require to see differenced time series

```{r}
#no. of seasonal difference
rpce_durable_goods_gdp %>% mutate(t_Expenditure = box_cox(Expenditure,lambda1)) %>%
  features(t_Expenditure, unitroot_nsdiffs)
#no. of regular difference
rpce_durable_goods_gdp %>% mutate(d_Expenditure = difference(box_cox(Expenditure,lambda1),4)) %>% features(d_Expenditure, unitroot_ndiffs)
```

* So, we need just one seasonal difference to make this data stationary as per above test results

```{r}
rpce_durable_goods_gdp %>%
  autoplot(box_cox(Expenditure,lambda1)%>% difference(lag=4))+ 
  ggtitle("Real personal consumption Expenditure: Durable Goods(Differenced timeseries)")+ylab("% of GDP")

#KPSS on box-cox transformed seasonal differenced time series
rpce_durable_goods_gdp %>% mutate(t_Expenditure = difference(box_cox(Expenditure,lambda1),4)) %>%
  features(t_Expenditure, unitroot_kpss)
```

* The seasonally differenced time series looks now to be stationary
* KPSS test also confirm our finding as the p value from above test is >0.05 and so we cannot reject null hypothesis and say that this differenced data is now stationary

```{r}
#checking residuals of differenced time series
rpce_durable_goods_gdp %>%
  gg_tsdisplay(box_cox(Expenditure,lambda1) %>% difference(lag=4), plot_type="partial")+
  ggtitle("Residuals of differenced RPCE: Durable Goods")
```

**ACF plot**

* We can see positive spikes at lag 1, 2, 3 and nothing after that. These are all non-seasonal spikes. This suggests of MA(1) or MA(2) or MA(3) non-seasonal component

**PACF plot**

* We can see a negative spike at lag 4, which is a seasonal spike and suggest AR(1) seasonal component
* A large spike is seen at lag 1, this is non-seasonal spikes. These are also seems to be decaying over time with largest significant spike seen at lag 1, which suggest AR(1) non-seasonal component
  * Model 1: ARIMA(0,0,3)(1,1,0)4
  * Model 2: ARIMA(0,0,1)(1,1,0)4
  * Model 3: ARIMA(1,0,0)(1,1,0)4
  * Model 4: ARIMA(0,0,2)(1,1,0)4
  

```{r}
#test data set
rpce_test <- rpce_durable_goods_gdp %>% 
  filter_index("2019 Q1" ~.)

#training data set
rpce_train <- rpce_durable_goods_gdp %>% 
  filter_index(~ "2018 Q4")

arima_all_models1 <- rpce_train %>%
  model(
    arima003110 = ARIMA(box_cox(Expenditure,lambda1) ~ pdq(0,0,3) + PDQ(1,1,0)),
    arima001110 = ARIMA(box_cox(Expenditure,lambda1) ~ pdq(0,0,1) + PDQ(1,1,0)),
    arima100110 = ARIMA(box_cox(Expenditure,lambda1) ~ pdq(1,0,0) + PDQ(1,1,0)),
    arima100011 = ARIMA(box_cox(Expenditure,lambda1) ~ pdq(0,0,2) + PDQ(1,1,0)),
    
    auto_arima = ARIMA(box_cox(Expenditure,lambda1), stepwise = FALSE, approx = FALSE)
  )

glance(arima_all_models1) %>% arrange(AICc)
```

* The lowest AICc is of auto_arima model which is a little better than our guessed model ARIMA(1,0,0)(1,1,0)
* So, we will take auto_arima model for our forecast

```{r}
best_arima_model1 <- arima_all_models1 %>% select(auto_arima)
report(best_arima_model1)
```

**Best ARIMA model-RPCE: ARIMA(1,0,0)(0,1,1)[4] w/ drift**

```{r}
#looking at residuals
best_arima_model1 %>%
  gg_tsresiduals()+ ggtitle("Residual analysis of RPCE Arima model")

#Doing Ljung box test on residuals
Box.test(augment(best_arima_model1)$.resid, lag=36, fitdf = 2, type = "Lj")
```

* **Observations**

  * By looking at distribution of the residuals, it looks like slightly skewed
  * There is also a constant variance seen in the time plot of residuals
  * The ACF plot shows no auto-correlation which is confirmed by performing Ljung Box test.        Here p-value >0.05, we cannot reject null hypothesis, and say that there is no                 autocorrelation and this is like white noise

<br>

* **Using ETS model**

  * Since this time series has a trend and seasonality in it so suggested ETS models will be:       ETS(A,A,A), ETS(A,Ad,A)
  * Since seasonality is increasing over time, it is good to see multiplicative models as well 
  * we will also look at ets model on seasonally adjusted data to get best of all models

```{r}
stl <- decomposition_model(
  STL(box_cox(Expenditure,lambda1)),
  ETS(season_adjust ~ error("A") + trend("A") + season("N"))
)

ets_all_models1 <- rpce_train %>%
  model(
    AAdA = ETS(box_cox(Expenditure,lambda1) ~ error("A") + trend("Ad") + season("A")),
    AAA = ETS(box_cox(Expenditure,lambda1) ~ error("A") +  trend("A") + season("A")),
    MAdM = ETS(box_cox(Expenditure,lambda1) ~ error("M") + trend("Ad") + season("M")),
    
    stl_ets = stl,
    ETS_auto = ETS(box_cox(Expenditure,lambda1))
    )
accuracy(ets_all_models1)
```

```{r}
ets_all_models1 %>%
  forecast(h = "4 years") %>%
  autoplot(rpce_durable_goods_gdp) +
  labs(y = "Expenditure(%of GDP)", title = "RPCE: Durable goods")
```

```{r}
ets_all_models1 %>%select(stl_ets) %>%
  forecast(h = "4 years") %>%
  autoplot(rpce_durable_goods_gdp) +
  labs(y = "Expenditure(%of GDP)", title = "RPCE: Durable goods")
```

* The STL decomposition forecasts using the additive trend model, ETS(A,A,N), is slightly better in-sample. However, note that this is a biased comparison as the models have different numbers of parameters. So, we will test for the accuracy of both the ets models on test data and select the best one.

**Best ETS model-PRCE: Decomposition model with additive trend : stl_ets with lowest RMSE**

```{r}
best_ets_model1_1 <- ets_all_models1 %>%
  select(stl_ets) 
best_ets_model1_2 <- ets_all_models1 %>%
  select(AAA) 
best_ets_model1_1 %>%
  gg_tsresiduals(lag_max = 24)+ggtitle("Residual analysis of RPCE Durable: ETS model")
best_ets_model1_2 %>%
  gg_tsresiduals(lag_max = 24)+ggtitle("Residual analysis of RPCE Durable: ETS model")
#Box pierce test for residuals
Box.test(augment(best_ets_model1_1)$.resid, lag=24, fitdf = 5, type = "Lj")
#Box pierce test for residuals
Box.test(augment(best_ets_model1_2)$.resid, lag=24, fitdf = 6, type = "Lj")
```

* **Observations**

  * The histograms of the residual is normal and centered around zero, which indicates that        probably the forecast from this method will be good, and also prediction intervals             computed will be accurate
  * The time plot of the residuals shows that the variation of the residuals stays                 approximately the same across the historical data, apart from some outliers
  * The residuals looks like white noise as there is no auto correlation seen which is             confirmed from the results of Ljung Box test, where p value >0.05 and hence we can say         residuals looks like white noise
  
* **Plotting both forecast**

```{r}
fc1 <- best_arima_model1 %>% forecast(h = "5 years")
fc2_1 <- best_ets_model1_1 %>% forecast(h = "5 years")
fc2_2 <- best_ets_model1_2 %>% forecast(h = "5 years")
r1<- fc1 %>%
  autoplot(rpce_train) +
  geom_line(data=rpce_test, aes(x=Quarter, y=Expenditure), col='red')+
  ggtitle("RPCE: Durable Goods: Arima")+ylab("Expenditure as % of GDP")
r2<- fc2_1 %>%
  autoplot(rpce_train) +
  geom_line(data=rpce_test, aes(x=Quarter, y=Expenditure), col='red')+
  ggtitle("RPCE: Durable Goods: ETS")+ylab("Expenditure as % of GDP")
r3<- fc2_2 %>%
  autoplot(rpce_train) +
  geom_line(data=rpce_test, aes(x=Quarter, y=Expenditure), col='red')+
  ggtitle("RPCE: Durable Goods: ETS")+ylab("Expenditure as % of GDP")
grid.arrange(r1,r2,r3,nrow=3)
```


* **Selecting best model for RPCE: Durable goods(ARIMA or ETS)**
```{r}
# Generate forecasts and compare accuracy over the test set
bind_rows(
    best_arima_model1 %>% accuracy(),
    best_ets_model1_1 %>% accuracy(),
    best_ets_model1_2 %>% accuracy(),
    best_arima_model1 %>% forecast(h = "4 years") %>% accuracy(rpce_test),
    best_ets_model1_1 %>% forecast(h = "4 years") %>% accuracy(rpce_test),
    best_ets_model1_2 %>% forecast(h = "4 years") %>% accuracy(rpce_test)
  ) %>%
  select(-ME, -MPE, -ACF1)
```

**So, since RMSE of ETS(A,A,A) comes out to be lowest on test data. Hence, ETS(A,A,A) is a better model. We will forecast the real personal consumption expenditure as % of GDP for 1 year ahead by using ETS(A,A,A) model**

<br>

**3.2.Industrial Production: Durable Goods**

<br>
**Looking at time series**
```{r}
#plotting graphs to see our time series
g1<- industrial_prod_durable_goods %>%
  autoplot(Index) + ggtitle("Industrial Production: Durable Goods") + ylab("Index")
g2<- industrial_prod_durable_goods %>%
  gg_season(Index) + ggtitle("Industrial Production: Durable Goods") + ylab("Index")
grid.arrange(g1, g2, nrow = 2)
```

**Observations for Industrial Production: Durable Goods**

* There is an upward trend in this series, with some exception periods. Like, 2008-2009 saw a drop in the industrial production. Which is expected as that was the time of great recession period
* Another drop in industrial production is seen in April 2020 - Jun 2020 which is when covid lockdown happened for the first time. But soon after that the production started rising. As mostly the demand increased for durable goods as people realized that staying at home is not going soon
* There is some seasonality seen in the series, like generally July seems to be a month of lower production every year, increases till October and then a slight downward trend

```{r}
#Checking for different components in the series
industrial_prod_durable_goods %>%
  model(
    STL(Index)) %>%
  components() %>%
  autoplot()+
  labs(title = "STL decomposition of Industrial Production: Consumer goods")
```
* So, there is a strong seasonality in the series which is increasing over time

**Forecast: Industrial production**

We will build two competitive models : ARIMA and ETS on training data and check the best performing model on test data set for every forecasting exercise we will do by looking at its accuracy

  * **Using Arima Model**
    
    * Step 1: Transform the series, if needed
    * Step 2: Check for stationarity by looking at ACF and running KPSS test
    * Step 3: Find number of seasonal and normal differencing needed in the data
    * Step 4: Plot differenced time series and run KPSS test again, check its residuals and ACF       and PACF plot
    * Step 5: Guess ARIMA models
    * Step 6: Divide data into test and train
    * Step 7: Run ARIMA models on train data and select the best model
    * Step 8: Check residuals of the model, run Ljung Box test
    * Step 9: Forecast  

```{r}
#transform the data and stabilize as there is variation in the series which is increasing with time
#Box-Cox transformation
lambda2 <- industrial_prod_durable_goods %>%
  features(Index, features = guerrero) %>%
  pull(lambda_guerrero)

g1<- industrial_prod_durable_goods %>%
  autoplot(box_cox(Index, lambda2)) +
  labs(y = "", title = TeX(paste0("Transformed Industrial Production for Durable Goods with $\\lambda$ = ", round(lambda2,2))))

g2<- industrial_prod_durable_goods %>%
  autoplot(log(Index)) +
  labs(y = "", title = "Log Transformed Industrial Production for Durable Goods")

#Step 2: Checking for stationary
g3<- industrial_prod_durable_goods %>% ACF(Index) %>% autoplot()+ ggtitle("ACF plot for Industrial production: Durable goods")

grid.arrange(g1,g2,g3, nrow=3)

#Running KPSS to statistically justify that data is stationary or not and find order of the differincing - Null hypothesis (Ho): Data is stationary - Alternate hypothesis (Halt): Data is non-stationary

industrial_prod_durable_goods %>% features(log(Index), unitroot_kpss)
```

* The time series is non-stationary as it clearly has trend and seasonality in it, which we observed in above graphs
* The ACF plot is decaying very slowly, which is indicative of non-stationarity in the time series
* Also, the p-value <0.05 in KPSS test, so we reject null hypothesis of KPSS test and can confirm that data is not stationary
* So, we require to see differenced time series

```{r}
#no. of seasonal difference - industrial production
industrial_prod_durable_goods %>% mutate(t_Index = log(Index)) %>%
  features(t_Index, unitroot_nsdiffs)

#no. of regular difference - industrial production
industrial_prod_durable_goods %>% mutate(d_Index = difference(log(Index),12)) %>% features(d_Index, unitroot_ndiffs)
```

* So, we need just one seasonal difference to make this data stationary as per above test results

```{r}
industrial_prod_durable_goods %>%
  autoplot(log(Index)%>% difference(lag=12))+ 
  ggtitle("Industrial Production: Durable Goods(Differenced timeseries)")+ylab("Index")

#KPSS on box-cox transformed differenced time series
industrial_prod_durable_goods %>% mutate(d_Index = difference(log(Index),12)) %>%  features(d_Index, unitroot_kpss)
```

* The seasonally differenced time series looks now to be stationary
* KPSS test also confirm our finding as the p value from above test is >0.05 and so we cannot     reject null hypothesis and say that this differenced data is now stationary

```{r}
#checking residuals of differenced time series
industrial_prod_durable_goods %>%
  gg_tsdisplay(log(Index) %>% difference(lag=12), plot_type="partial")+
  ggtitle("Residuals of differenced Industrial Production: Durable Goods")
```

**ACF**

* We can see negative spike at lag 12. This is seasonal spikes. Further these seasonal spike seems to be decaying exponentially over time. It suggest MA(1) seasonal component
* Significant autocorrelation is seen at lag 1 which is decaying exponentially over time. This is non- seasonal spike. It suggests non-seasonal MA(1) component

**PACF**

* We can see negative spike at lag 12. This is seasonal spikes. Further these seasonal spike seems to be decaying exponentially over time. This suggest AR(1) seasonal component
* Significant partial autocorrelation is seen at lag 1. This is non- seasonal spike which suggest AR(1) non-seasonal component

* Moodels suggested : 
  * Model 1: ARIMA(1,0,0)(1,1,0)12
  * Model 2: ARIMA(0,0,1)(0,1,1)12
  * Model 3: ARIMA(1,0,0)(0,1,1)12
  * Model 4: ARIMA(0,0,1)(1,1,0)12


```{r}
#test data set
ip_test <- industrial_prod_durable_goods %>% 
  filter_index("2019 Jan" ~.)

#training data
ip_train <- industrial_prod_durable_goods %>% 
  filter_index(~ "2018 Dec")

arima_all_models2 <- ip_train %>%
  model(
    arima100110 = ARIMA(log(Index) ~ pdq(1,0,0) + PDQ(1,1,0)),
    arima001011 = ARIMA(log(Index) ~ pdq(0,0,1) + PDQ(0,1,1)),
    arima100011 = ARIMA(log(Index) ~ pdq(1,0,0) + PDQ(0,1,1)),
    arima001110 = ARIMA(log(Index) ~ pdq(0,0,1) + PDQ(1,1,0)),
    
    auto_arima1 = ARIMA(log(Index), stepwise = FALSE, approx = FALSE)
  )

glance(arima_all_models2) %>% arrange(AICc)
```

* The lowest AICc is of auto_arima1 model which is a little better than our guessed model ARIMA(1,0,0)(0,1,1)
* So, we will take auto_arima1 model for our forecast

```{r}
best_arima_model2 <- arima_all_models2 %>% select(auto_arima1)
report(best_arima_model2)
```

**Best ARIMA model-Industrial Production: ARIMA(2,0,2)(0,1,1)[12] w/ drift **

```{r}
#looking at residuals
best_arima_model2 %>%
  gg_tsresiduals()+ ggtitle("Residual analysis of Industrial Production Arima model")

#Doing Ljung box test on residuals
Box.test(augment(best_arima_model2)$.resid, lag=36, fitdf = 5, type = "Lj")
```

* **Observations**

  * By looking at distribution of the residuals, it looks like normal
  * There is also a constant variance seen in the time plot of residuals
  * The ACF plot shows no auto-correlation which is confirmed by performing Ljung Box test.        Here p-value>0.05, we do reject null hypothesis, and say that there is no autocorrelation      and residuals do look like white noise
  
<br>

* **Using ETS Model**

  * Since, the industrial production for durable goods time series is seasonal and has trend in     it, we think it should be ETS(A,Ad,A)
  * Since seasonality is increasing over time, it is good to see multiplicative models as well     :ETS(M,Ad,M) model
  * Let's test the possible models

```{r}
ets_all_models2 <- ip_train %>%
  model(
    AAdA = ETS(log(Index) ~ error("A") + trend("Ad") + season("A")),
    AAA = ETS(log(Index) ~ error("A") +  trend("A") + season("A")),
    MAM = ETS(log(Index) ~ error("M") +  trend("A") + season("M")),
    MAdM = ETS(log(Index) ~ error("M") +  trend("Ad") + season("M")),
    
    ETS = ETS(log(Index))
    )
accuracy(ets_all_models2)
```


```{r}
ets_all_models2 %>%
  forecast(h = "4 years") %>%
  autoplot(industrial_prod_durable_goods %>% filter(year(Month)>2015)) +
  labs(y = "Index", title = "Industrial Production: Durable goods")
```


**Best ETS Model- Industrial Production: ETS(M,Ad,M)**

```{r}
best_ets_model2 <- ets_all_models2 %>%
  select(MAdM) 

best_ets_model2 %>%
  gg_tsresiduals(lag_max = 24)+ggtitle("Residual analysis of Industrial production Durable: ETS model")

#Doing Ljung box test on residuals
Box.test(augment(best_ets_model2)$.resid, lag=36, fitdf = 17, type = "Lj")

```

* **Observations**

  * By looking at distribution of the residuals, it looks like normal
  * There is also a constant variance seen in the time plot of residuals
  * The ACF plot shows  auto-correlation which is confirmed by performing Ljung Box test. Here      p-value <0.05, we reject null hypothesis, and say that there is autocorrelation and             residuals do not look like white noise

* **Plotting both forecast**

```{r}
fc3 <- best_arima_model2 %>% forecast(h = "3 years")
fc4 <- best_ets_model2%>% forecast(h = "3 years")

i1<- fc3 %>%
  autoplot(ip_train %>% filter(year(Month)>2015)) +
  geom_line(data=(ip_test%>% filter(year(Month)>2015)), aes(x=Month, y=(Index)), col='red')+
  ggtitle("Industrial Production: Durable Goods: Arima")+ylab("Index")
i2<- fc4 %>%
  autoplot(ip_train%>% filter(year(Month)>2015)) +
  geom_line(data=(ip_test%>% filter(year(Month)>2015)), aes(x=Month, y=(Index)), col='red')+
  ggtitle("Industrial Production: Durable Goods: ETS")+ylab("Index")

grid.arrange(i1,i2,nrow=2)
```


* **Selecting best model for Industrial Production: Durable goods(ARIMA or ETS)**

```{r}
# Generate forecasts and compare accuracy over the test set
bind_rows(
    best_arima_model2 %>% accuracy(),
    best_ets_model2 %>% accuracy(),
  
    best_arima_model2 %>% forecast(h = "4 years") %>% accuracy(ip_test),
    best_ets_model2%>% forecast(h = "4 years") %>% accuracy(ip_test)
  ) %>%
  select(-ME, -MPE, -ACF1)
```

**So, since RMSE of ETS(M,Ad,M) comes out to be lowest on test data. Hence, ETS(M,Ad,M) is a better model. We will forecast the industrial production of durable goods for 1 year ahead by using ETS(M,Ad,M) model**

<br>

**3.3.Producer Price Index by Commodity: Durable Goods**

<br>

**Looking at time series**
```{r}
#plotting graphs to see our time series
g1<- PPI_durable_goods %>%
  autoplot(Index) + ggtitle("Producer Price Index: Durable Goods") + ylab("Index")
g2<- PPI_durable_goods %>%
  gg_season(Index) + ggtitle("Producer Price Index: Durable Goods") + ylab("Index")
grid.arrange(g1, g2, nrow = 2)
```

**Observations for Producer Price Index by Commodities:Final Demand: Durable Goods**

* There is an upward trend in this series, with some exception periods, like, 2008-2009. Which is expected as that was the time of great recession period
* The series sees a slight increase in PPI index towards the end of the year
* Though 2021 has seen a sharp increase in the index
* There is no seasonality seen but lets look at it stl decomposition

```{r}
#Checking for different components in the series
PPI_durable_goods %>%
  model(
    STL(Index)) %>%
  components() %>%
  autoplot()+
  labs(title = "STL decomposition of PPI: Consumer goods")
```

* So we could see an upward trend and seasonality changing with time in the series. Seasonality is very very low between 0.2 to -0.2 towards end of data


**Forecast: PPI-Durable goods**

We will build two competitive models : ARIMA and ETS on training data and check the best performing model on test data set for every forecasting exercise we will do by looking at its accuracy

  * **Using ARIMA**
    
    * Step 1: Transform the series, if needed
    * Step 2: Check for stationarity by looking at ACF and running KPSS test
    * Step 3: Find number of seasonal and normal differencing needed in the data
    * Step 4: Plot differenced time series and run KPSS test again, check its residuals and ACF       and PACF plot
    * Step 5: Guess ARIMA models
    * Step 6: Divide data into test and train
    * Step 7: Run ARIMA models on train data and select the best model
    * Step 8: Check residuals of the model, run Ljung Box test
    * Step 9: Forecast

```{r}
#We will only take log transformations as we are looking into the growths
#Step 2: Checking for stationary
PPI_durable_goods %>% ACF(Index) %>% autoplot()+ggtitle("ACF plot for PPI: Durable goods")

#Running KPSS to statistically justify that data is stationary or not and find order of the differincing - Null hypothesis (Ho): Data is stationary - Alternate hypothesis (Halt): Data is non-stationary
PPI_durable_goods %>% features(log(Index), unitroot_kpss)
```

* The time series is non-stationary as it clearly has trend in it, which we observed in above graphs
* The ACF plot is decaying very slowly, which is indicative of non-stationarity in the time series
* Also, the p-value <0.05 in KPSS test, so we reject null hypothesis of KPSS test and can confirm that data is not stationary
* So, we require to see differenced time series


```{r}
#no. of seasonal difference - PPI
PPI_durable_goods %>% mutate(t_Index = log(Index)) %>%
  features(t_Index, unitroot_nsdiffs)

#no. of regular difference - PPI
PPI_durable_goods %>%  features(log(Index), unitroot_ndiffs)
```

* So, we need just two normal difference to make this data stationary as per above test results

```{r}
PPI_durable_goods %>%
  autoplot(log(Index)%>% difference() %>% difference())+ 
  ggtitle("PPI: Durable Goods(Differenced timeseries)")+ylab("Index")

#KPSS on box-cox transformed differenced time series
PPI_durable_goods %>% mutate(d_Index = difference(difference(log(Index)))) %>%  features(d_Index, unitroot_kpss)
```


* The double differenced time series looks now to be stationary
* KPSS test also confirm our finding as the p value from above test is >0.05 and so we cannot reject null hypothesis and say that this differenced data is now stationary


```{r}
#checking residuals of differenced time series
PPI_durable_goods %>%
  gg_tsdisplay(log(Index) %>% difference() %>% difference(), plot_type="partial", lag_max = 36)+
  ggtitle("Residuals of differenced PPI: Durable Goods")
```


**ACF**

* There is a significant negative spike at lag 1, which suggest non-seasonal MA(1) component


**PACF**

* The lags can be seen to be decaying exponentially. There is some negative partial autocorrelation at lag1, lag2, lag3 , lag4, and lag5. We can consider lag 1 and lag2 to be significant, which suggests AR(1) or AR(2) non-seasonal components

* Suggested Model
  * ARIMA(0,2,1)
  * ARIMA(1,2,0)
  * ARIMA(0,2,2)
  * ARIMA(2,2,0)

```{r}
#test data set
ppi_test <- PPI_durable_goods %>% 
  filter_index("2019 Jan" ~.)

ppi_train <- PPI_durable_goods %>% 
  filter_index(~ "2018 Dec")

arima_all_models3 <- ppi_train %>%
  model(
    arima021= ARIMA(log(Index) ~ pdq(0,2,1) + PDQ(0,0,0)),
    arima120 = ARIMA(log(Index) ~ pdq(1,2,0) + PDQ(0,0,0)),
    arima022 = ARIMA(log(Index) ~ pdq(0,2,2) + PDQ(0,0,0)),
    arima220= ARIMA(log(Index) ~ pdq(2,2,0) + PDQ(0,0,0)),
    arima123= ARIMA(log(Index) ~ pdq(1,2,3) + PDQ(0,0,0)),
    
    arima_model = ARIMA(log(Index), stepwise = FALSE, approx = FALSE)
    
  )
glance(arima_all_models3) %>% arrange(AICc)
```


* The lowest AICc is of auto arima_model model which is close to our picked models
* So, we will use aimra_model for our forecast as the best model

```{r}
best_arima_model3 <- arima_all_models3 %>% select(arima_model)
report(best_arima_model3)
```

* We could see that auto arima model is picking up a model with seasonal factor in it. It could be because we see a significant spike on lag 24 which it is calculating as a seasonal spike in the data

**Best ARIMA model-PPI: ARIMA(0,2,2)(2,0,2)[12] **

```{r}
#looking at residuals
best_arima_model3 %>%
  gg_tsresiduals()+ ggtitle("Residual analysis of PPI Arima model")

#Doing Ljung box test on residuals
Box.test(augment(best_arima_model3)$.resid, lag=36, fitdf = 6, type = "Lj")
```


* **Observations**

  * By looking at distribution of the residuals, it looks like normal
  * There is not a constant variance seen in the time plot of residuals
  * The ACF plot shows some auto-correlation which is confirmed by performing Ljung Box            test. Here p-value < 0.05, we reject null hypothesis, and say that there is                    auto-correlation and residuals do not look like white noise

<br>

* **Using ETS Model**

  * Since, the production price index for durable goods time series has trend in it with little     to no seasonality, we think it should be ETS(A,A,N), ETS(A,Ad,N), ETS(M,Ad,N) or ETS(M,A,N)     model. Let’s test the possible models

```{r}
stl <- decomposition_model(
  STL(Index),
  ETS(season_adjust ~ error("A") + trend("Ad") + season("N"))
)
ets_all_models3 <- ppi_train %>%
  model(
    snaive = SNAIVE(log(Index)),
    AAdN = ETS(log(Index) ~ error("A") + trend("Ad") + season("N")),
    AAN = ETS(log(Index) ~ error("A") +  trend("A") + season("N")),
    MAN = ETS(log(Index) ~ error("M") +  trend("A") + season("N")),
    MAdN = ETS(log(Index) ~ error("M") +  trend("Ad") + season("N")),
    ETS = ETS(log(Index)),
    ETS1 = ETS(log(Index)),
    stl_ets = stl
    )
accuracy(ets_all_models3)
```

```{r}
ets_all_models3 %>%
  forecast(h = "6 years") %>%
  autoplot(PPI_durable_goods) +
  labs(y = "Expenditure(%of GDP)", title = "RPCE: Durable goods")
```



**Best ETS Model- PPI Durable goods: ETS(A,Ad,N) on seasonally adjusted data-stl_ets**

```{r}
best_ets_model3 <- ets_all_models3 %>%
  select(stl_ets)
best_ets_model3 %>%
  gg_tsresiduals(lag_max = 24)+ggtitle("Residual analysis of PPI Durable: ETS model")

#Doing Ljung box test on residuals
Box.test(augment(best_ets_model3)$.resid, lag=36, fitdf = 5, type = "Lj")
```

* **Observations**

  * By looking at distribution of the residuals, it looks like normal
  * There is not constant variance seen in the time plot of residuals
  * The ACF plot shows auto-correlation which is confirmed by performing Ljung Box test. Here       p-value <0.05, we reject null hypothesis, and say that there is autocorrelation and             residuals do not look like white noise

* **Plotting both forecasts**

```{r}
fc5 <- best_arima_model3 %>% forecast(h = "5 years")
fc6 <- best_ets_model3 %>% forecast(h = "5 years")
p1<- fc5 %>%
  autoplot(ppi_train %>% filter(year(Month)>2000)) +
  geom_line(data=(ppi_test%>% filter(year(Month)>2000)), aes(x=Month, y=Index), col='red')+
  ggtitle("PPI: Durable Goods: ARIMA")+ylab("Index")
p2<- fc6 %>%
  autoplot(ppi_train%>% filter(year(Month)>2000)) +
  geom_line(data=(ppi_test%>% filter(year(Month)>2000)), aes(x=Month, y=Index), col='red')+
  ggtitle("PPI: Durable Goods: ETS")+ylab("Index")
grid.arrange(p1,p2,ncol=2)
```


* **Selecting best model for PPI: Durable goods(ARIMA or ETS)**

```{r}
# Generate forecasts and compare accuracy over the test set
bind_rows(
    best_arima_model3 %>% accuracy(),
    best_ets_model3 %>% accuracy(),
    best_arima_model3 %>% forecast(h = "4 years") %>% accuracy(ppi_test),
    best_ets_model3 %>% forecast(h = "4 years") %>% accuracy(ppi_test)
  ) %>%
  select(-ME, -MPE, -ACF1)
```


**Clearly, ARIMA model performed better than ETS on test data here as RMSE of arima model is lesser. So we will use ARIMA model to forecast PPI**

<br>

**3.4.Consumer Price Index for all Urban Consumers: Durable Goods**

<br>
**Looking at time series**
```{r}
#plotting graphs to see our time series
g1<- CPI_durable_goods %>%
  autoplot(Index) + ggtitle("Consumer Price Index: Durable Goods")
g2<- CPI_durable_goods %>%
  gg_season(Index) + ggtitle("Consumer Price Index: Durable Goods")
grid.arrange(g1, g2, nrow = 2)
```

**Observations for Consumer Price Index for all Urban Consumers: Durable Goods**

* This series clearly has an upwars trend up until 1996 and start dropping down from 1997 Jan
* But the trend started to pick upward from Jun 2020, 3 months after country went into lock down
* Overall no seasonality is seen


**Forecast: CPI-durable goods**

We will build two competitive models : ARIMA and ETS on training data and check the best performing model on test data set for every forecasting exercise we will do by looking at its accuracy

  * **Using ARIMA Model**

    * Step 1: Transform the series, if needed
    * Step 2: Check for stationarity by looking at ACF and running KPSS test
    * Step 3: Find number of seasonal and normal differencing needed in the data
    * Step 4: Plot differenced time series and run KPSS test again, check its residuals and ACF       and PACF plot
    * Step 5: Guess ARIMA models
    * Step 6: Divide data into test and train
    * Step 7: Run ARIMA models on train data and select the best model
    * Step 8: Check residuals of the model, run Ljung Box test
    * Step 9: Forecast


```{r}
#we will se at log transformed series
CPI_durable_goods %>%
  autoplot(log(Index)) +
  labs(y = "", title = "Log Transformed Consumer Price Index for Durable Goods")

#Step 2: Checking for stationary
CPI_durable_goods %>% ACF(Index) %>% autoplot()+ggtitle("ACF plot for CPI: Durable goods")

#Running KPSS to statistically justify that data is stationary or not and find order of the differincing - Null hypothesis (Ho): Data is stationary - Alternate hypothesis (Halt): Data is non-stationary
CPI_durable_goods %>% features(log(Index), unitroot_kpss)
```

* The time series is non-stationary as it clearly has trend in it, which we observed in above graphs
* The ACF plot is decaying very slowly, which is indicative of non-stationarity in the time series
* Also, the p-value <0.05 in KPSS test, so we reject null hypothesis of KPSS test and can confirm that data is not stationary
* So, we require to see differenced time series

```{r}
#no. of seasonal difference - Consumer price index
CPI_durable_goods %>% mutate(t_Index = log(Index)) %>%
  features(t_Index, unitroot_nsdiffs)
#no. of regular difference - industrial production
CPI_durable_goods %>%  features(log(Index), unitroot_ndiffs)
```
* So, we need just two normal difference to make this data stationary as per above test results

```{r}
CPI_durable_goods %>%
  autoplot(log(Index)%>% difference() %>% difference())+ 
  ggtitle("CPI: Durable Goods(Differenced timeseries)")+ylab("Index")

#KPSS on box-cox transformed differenced time series
CPI_durable_goods %>% mutate(d_Index = difference(difference(log(Index)))) %>%  features(d_Index, unitroot_kpss)
```

* The double differenced time series looks now to be stationary
* KPSS test also confirm our finding as the p value from above test is >0.05 and so we cannot reject null hypothesis and say that this differenced data is now stationary

```{r}
#checking residuals of differenced time series
CPI_durable_goods %>%
  gg_tsdisplay(log(Index) %>% difference() %>% difference(), plot_type="partial", lag_max = 36)+ 
  ggtitle("Residuals of differenced CPI: Durable Goods")
```


**ACF**

* There is a significant negative spike at lag 1 and lag 2, which suggest MA(1) or MA(2)component 

**PACF**

* The lags can be seen to be decaying exponentially.


* Suggested Model: ARIMA(0,2,1) or ARIMA(1,2,1) or ARIMA(0,2,2)

```{r}
#test data set
cpi_test <- CPI_durable_goods %>% 
  filter_index("2019 Jan" ~.)

cpi_train <- CPI_durable_goods %>% 
  filter_index(~ "2018 Dec")

arima_all_models4 <- cpi_train %>%
  model(
    arima021 = ARIMA(log(Index) ~ pdq(0,2,1) + PDQ(0,0,0)),
    arima121 = ARIMA(log(Index) ~ pdq(1,2,1) + PDQ(0,0,0)),
    arima120 = ARIMA(log(Index) ~ pdq(1,2,0) + PDQ(0,0,0)),
    arima022 = ARIMA(log(Index) ~ pdq(0,2,2) + PDQ(0,0,0)),
   arima_model = ARIMA(log(Index), stepwise = FALSE, approx = FALSE)
    
  )
glance(arima_all_models4) %>% arrange(AICc)

```


* The lowest AICc is of auto arima_model which is close to our guess ARIMA(1,2,1) model. So, we will take auto arima_model model for our forecast

```{r}
best_arima_model4 <- arima_all_models4 %>% select(arima_model)
report(best_arima_model4)
```

* We can see that auto arima model has MA(2) seasonal component because it is capturing the spike at lag 24 in ACF plot which can be seasonal spike for it


**Best ARIMA model-CPI: ARIMA(1,2,1)(0,0,2)[12]**

```{r}
#looking at residuals
best_arima_model4 %>%
  gg_tsresiduals()+ ggtitle("Residual analysis of CPI Arima model")
#Doing Ljung box test on residuals
Box.test(augment(best_arima_model4)$.resid, lag=36, fitdf = 2, type = "Lj")
```


* **Observations**

  * By looking at distribution of the residuals, it looks like normal 
  * There is not a constant variance seen in the time plot of residuals 
  * The ACF plot shows some auto-correlation which is confirmed by performing Ljung Box test.       Here p-value <0.05, we reject null hypothesis, and say that there is autocorrelation and        residuals do not look like white noise

* **Using ETS model**

  * Since, the CPI for durable goods time series has trend in it, we think it should be             ETS(A,A,N), ETS(A,Ad,N), ETS(M,Ad,N) or ETS(M,A,N) model. Let’s test the possible models


```{r}
stl2 <- decomposition_model(
  STL(log(Index)),
  ETS(season_adjust ~ error("A") + trend("Ad") + season("N"))
)

ets_all_models4 <- cpi_train %>%
  model(AAdN = ETS(log(Index) ~ error("A") + trend("Ad") + season("N")),
    AAN = ETS(log(Index) ~ error("A") +  trend("A") + season("N")),
    MAN = ETS(log(Index) ~ error("M") +  trend("A") + season("N")),
    MAdN = ETS(log(Index) ~ error("M") +  trend("Ad") + season("N")),
    
    stl_ets = stl2,
    ETS = ETS(log(Index))
    )
accuracy(ets_all_models4)
```
* The STL decomposition forecasts using the additive trend model, ETS(A,Ad,N), is slightly better in-sample. However, note that this is a biased comparison as the models have different numbers of parameters. So, we will test for the accuracy of both the ets models on test data and select the best one.


**Best ETS Model-CPI: Durable goods: ETS(A,Ad,N) and stl decomposition ETS(A,Ad,N)**

```{r}
best_ets_model4_1 <- ets_all_models4 %>%
  select(AAdN)
best_ets_model4_2 <- ets_all_models4 %>%
  select(stl_ets)
best_ets_model4_1 %>%
  gg_tsresiduals(lag_max = 24)+ggtitle("Residual analysis of CPI Durable: ETS model")
#Doing Ljung box test on residuals
Box.test(augment(best_ets_model4_1)$.resid, lag=36, fitdf = 5, type = "Lj")

best_ets_model4_2 %>%
  gg_tsresiduals(lag_max = 24)+ggtitle("Residual analysis of CPI Durable: ETS model")
#Doing Ljung box test on residuals
Box.test(augment(best_ets_model4_2)$.resid, lag=36, fitdf = 5, type = "Lj")
```

* **Observations**

  * By looking at distribution of the residuals, it looks like normal
  * There is not constant variance seen in the time plot of residuals
  * The ACF plot shows auto-correlation which is confirmed by performing Ljung Box test. Here       p-value <0.05, we reject null hypothesis, and say that there is autocorrelation and             residuals do not look like white noise
  
* **Plotting both forecasts**

```{r}
fc7 <- best_arima_model4 %>% forecast(h = "5 years")
fc8_1 <- best_ets_model4_1 %>% forecast(h = "5 years")
fc8_2 <- best_ets_model4_2 %>% forecast(h = "5 years")
c1<- fc7 %>%
  autoplot(cpi_train ) +
  geom_line(data=cpi_test, aes(x=Month, y=Index), col='red')+
  ggtitle("CPI: Durable Goods: ARIMA")+ylab("Index")
c2<- fc8_1 %>%
  autoplot(cpi_train) +
  geom_line(data=cpi_test, aes(x=Month, y=Index), col='red')+
  ggtitle("CPI: Durable Goods: ETS")+ylab("Index")
c3<- fc8_2 %>%
  autoplot(cpi_train) +
  geom_line(data=cpi_test, aes(x=Month, y=Index), col='red')+
  ggtitle("CPI: Durable Goods: ETS")+ylab("Index")
grid.arrange(c1,c2,c3, nrow =3)
```

* **Selecting best model for CPI: Durable goods(ARIMA or ETS)**

```{r}
# Generate forecasts and compare accuracy over the test set
bind_rows(
    best_arima_model4 %>% accuracy(),
    best_ets_model4_1 %>% accuracy(),
    best_ets_model4_2 %>% accuracy(),
    best_arima_model4 %>% forecast(h = "3 years") %>% accuracy(cpi_test),
    best_ets_model4_1 %>% forecast(h = "3 years") %>% accuracy(cpi_test),
    best_ets_model4_2 %>% forecast(h = "3 years") %>% accuracy(cpi_test)
  ) %>%
  select(-ME, -MPE, -ACF1)
```

**Clearly, ETS(A,Ad,N) model performed better than ARIMA on test data here as RMSE of ETS model is lesser. So we will use ETS model to forecast CPI**

<br>

**3.5.Real Disposable Income**

**Looking into the series**
```{r}
rdpi <- data_rdpi %>% mutate(Income = DPIC96) %>% select(Quarter,Income)
#plotting graphs to see our time series
g1<- rdpi %>%
  autoplot(Income) + ggtitle("Real personal disposable income") 
g2<- rdpi %>%
  gg_season(Income) + ggtitle("Real personal disposable income")
g3<- rdpi %>%
  gg_subseries(Income) + ggtitle("Real personal disposable income")
grid.arrange(g1, g2,g3, nrow = 3)
```
<br>
**Observations for Real Personal Consumption Expenditure: Durable Goods**

* There is an upward trend in this series with no seasonality

**Forecast: real disposable income**

We will build two competitive models : ARIMA and ETS on training data and check the best performing model on test data set for every forecasting exercise we will do by looking at its accuracy

* **Using ARIMA Model**

  * Step 1: Transform the series, if needed
  * Step 2: Check for stationarity by looking at ACF and running KPSS test
  * Step 3: Find number of seasonal and normal differencing needed in the data
  * Step 4: Plot differenced time series and run KPSS test again, check its residuals and ACF       and PACF plot
  * Step 5: Guess ARIMA models 
  * Step 6: Divide data into test and train
  * Step 7: Run ARIMA models on train data and select the best model
  * Step 8: Check residuals of the model, run Ljung Box test
  * Step 9: Forecast
  
```{r}
#Checking for stationary
rdpi %>% ACF(Income) %>% autoplot()+ggtitle("ACF plot for Real Disposable income")

#Running KPSS to statistically justify that data is stationary or not and find order of the differincing - Null hypothesis (Ho): Data is stationary - Alternate hypothesis (Halt): Data is non-stationary
rdpi %>% features(Income, unitroot_kpss)

```
* The time series is non-stationary as it clearly has trend in it, which we observed in above graphs
* The ACF plot is decaying very slowly, which is indicative of non-stationarity in the time series
* Also, the p-value <0.05 in KPSS test, so we reject null hypothesis of KPSS test and can confirm that data is not stationary 
* So, we require to see differenced time series

```{r}
#no. of seasonal difference
rdpi %>% mutate(t_Income = Income) %>%
  features(t_Income, unitroot_nsdiffs)
#no. of regular difference
rdpi %>% mutate(t_Income = Income) %>% features(t_Income, unitroot_ndiffs)
```
* So, we need just two normal difference to make this data stationary as per above test results

```{r}
rdpi %>%
  autoplot(Income %>% difference()%>%difference())+ 
  ggtitle("Real Disposable Personal Income(Differenced timeseries)")

#KPSS on box-cox transformed seasonal differenced time series
rdpi %>% mutate(t_Income = difference(difference(Income))) %>%
  features(t_Income, unitroot_kpss)
```
* The seasonally differenced time series looks now to be stationary
* KPSS test also confirm our finding as the p value from above test is >0.05 and so we cannot reject null hypothesis and say that this differenced data is now stationary


```{r}
#checking residuals of differenced time series
rdpi %>%
  gg_tsdisplay(Income %>% difference()%>% difference(), plot_type="partial")+
  ggtitle("Residuals of differenced RDPI")
```


**ACF plot**

* We can see positive spikes at lag 1, 3 and nothing after that. These are all non-seasonal spikes. This suggests of MA(1) or MA(3) non-seasonal component

**PACF plot**

* A large spike is seen at lag 2, this is non-seasonal spikes. These are also seems to be decaying over time with largest significant spike seen at lag 2, which suggest AR(2) non-seasonal component
  * Model 1: ARIMA(0,2,3)
  * Model 2: ARIMA(0,2,1)
  * Model 3: ARIMA(2,2,0)

```{r}
#test data set
rdpi_test <- rdpi %>% 
  filter_index("2019 Q1" ~.)

#training data set
rdpi_train <- rdpi %>% 
  filter_index(~ "2018 Q4")

arima_all_models_rdpi <- rdpi_train %>%
  model(
    arima023 = ARIMA(Income ~ pdq(0,2,3) + PDQ(0,0,0)),
    arima021 = ARIMA(Income ~ pdq(0,2,1) + PDQ(0,0,0)),
    arima220 = ARIMA(Income ~ pdq(2,2,0) + PDQ(0,0,0)),
    
    auto_arima = ARIMA(Income, stepwise = FALSE, approx = FALSE)
  )

glance(arima_all_models_rdpi) %>% arrange(AICc)
```
  
* The lowest AICc is of auto_arima model which is a little better than our guessed model ARIMA(0,2,3)

```{r}

report(arima_all_models_rdpi %>% select(auto_arima))
```

* The auto arima model picked up a seasonal component AR(1) due to the spike seen at lag4 in PACF plot. Though data is non seasonal but this model picked up the information at that lag
* Since the models are not very different, we will take best model as ARIMA(0,2,3)

**Best ARIMA model-RDPI: ARIMA(0,2,3)**

```{r}
best_arima_model_rdpi<- arima_all_models_rdpi %>% select(arima023)
#looking at residuals
best_arima_model_rdpi %>%
  gg_tsresiduals()+ ggtitle("Residual analysis of RDPI Arima model")

#Doing Ljung box test on residuals
Box.test(augment(best_arima_model_rdpi)$.resid, lag=36, fitdf = 3, type = "Lj")
```

* **Observations**

  * By looking at distribution of the residuals, it looks like slightly skewed
  * There is also no constant variance seen in the time plot of residuals
  * The ACF plot shows some auto-correlation which is confirmed by performing Ljung Box test.        Here p-value <0.05, we  reject null hypothesis, and say that there is                          autocorrelation and this is not like white noise

<br>

* **Using ETS model**

  * Since this time series has a trend  in it so suggested ETS models will be:       ETS(A,A,N), ETS(A,Ad,N)
  * we will also look at ets model on stl decomposed seasonally adjusted data to get best of all models
  
```{r}
stl_rd <- decomposition_model(
  STL(Income),
  ETS(season_adjust ~ error("A") + trend("A") + season("N"))
)

ets_all_models_rd <- rdpi_train %>%
  model(
    AAdN = ETS(Income ~ error("A") + trend("Ad") + season("N")),
    AAN = ETS(Income ~ error("A") +  trend("A") + season("N")),
    
    stl_ets = stl_rd,
    ETS_auto = ETS(Income)
    )
accuracy(ets_all_models_rd)
```

* The STL decomposition forecasts using the additive trend model, ETS(A,A,N), is slightly better in-sample. However, note that this is a biased comparison as the models have different numbers of parameters. So, we will test for the accuracy of both the ets models on test data and select the best one.

**Best ETS model-RDPI: Decomposition model with additive trend : stl_ets with lowest RMSE**


```{r}
best_ets_model_rd_1 <- ets_all_models_rd %>%
  select(stl_ets) 
best_ets_model_rd_2 <- ets_all_models_rd %>%
  select(AAN) 
```

* **Selecting best model for RDPI (ARIMA or ETS)**
```{r}
# Generate forecasts and compare accuracy over the test set
bind_rows(
    best_arima_model_rdpi %>% accuracy(),
    best_ets_model_rd_1 %>% accuracy(),
    best_ets_model_rd_2 %>% accuracy(),
    best_arima_model_rdpi %>% forecast(h = "4 years") %>% accuracy(rdpi_test),
    best_ets_model_rd_1 %>% forecast(h = "4 years") %>% accuracy(rdpi_test),
    best_ets_model_rd_2 %>% forecast(h = "4 years") %>% accuracy(rdpi_test)
  ) %>%
  select(-ME, -MPE, -ACF1)
```

**So, since RMSE of ETS(A,A,N) comes out to be lowest on test data. Hence, ETS(A,A,N) is a better model. We will forecast the real personal disposable income for 1 year ahead by using ETS(A,A,N) model**



### 4.Non-Durable Goods - Modelling

**4.1.Real Personal Consumption Expenditure: Non Durable Goods**

**Looking at time series**

```{r}
#plotting graphs to see our time series
g1<- rpce_ndurable_goods_gdp %>%
  autoplot(Expenditure) + ggtitle("Real personal consumption Expenditure: Non Durable Goods") + ylab("% of GDP")
g2<- rpce_ndurable_goods_gdp %>%
  gg_season(Expenditure) + ggtitle("Real personal consumption Expenditure: Non Durable Goods")+ylab("% of GDP")
g3<- rpce_ndurable_goods_gdp %>%
  gg_subseries(Expenditure) + ggtitle("Real personal consumption Expenditure: Non Durable Goods")+ylab("% of GDP")
grid.arrange(g1, g2,g3, nrow = 3)
```



```{r}
#Checking for different components in the series
rpce_ndurable_goods_gdp %>%
  model(
    STL(Expenditure)) %>%
  components() %>%
  autoplot()+
  labs(title = "STL decomposition of RPCE: Non durable goods")
```

<br>

**Forecast: RPCE**

```{r}

#Checking for stationary
rpce_ndurable_goods_gdp %>% ACF(Expenditure) %>% autoplot()+ggtitle("ACF plot for Real Personal Consumption Expenditure: Non-Durable goods")

```


Since data has trend and seasonality so it is non-stationary data.

```{r}
#no. of seasonal difference
rpce_ndurable_goods_gdp %>% mutate(t_Expenditure = Expenditure) %>%
  features(t_Expenditure, unitroot_nsdiffs)
#no. of regular difference
rpce_ndurable_goods_gdp %>% mutate(d_Expenditure = difference(Expenditure,4)) %>% features(d_Expenditure, unitroot_ndiffs)
```

* So, we need just one seasonal difference and one normal difference to make this data stationary as per above test results

```{r}
rpce_ndurable_goods_gdp %>%
  autoplot(Expenditure%>% difference(lag=4)%>% difference())+ 
  ggtitle("Real personal consumption Expenditure: NOn-Durable Goods(Differenced timeseries)")+ylab("% of GDP")

#KPSS on box-cox transformed seasonal differenced time series
rpce_ndurable_goods_gdp %>% mutate(t_Expenditure = difference(difference(Expenditure,4))) %>%
  features(t_Expenditure, unitroot_kpss)
```


* The double differenced time series looks now to be stationary
* KPSS test also confirm our finding as the p value from above test is >0.05 and so we cannot reject null hypothesis and say that this differenced data is now stationary

```{r}
#checking residuals of differenced time series
rpce_ndurable_goods_gdp %>%
  gg_tsdisplay(Expenditure %>% difference(lag=4)%>%difference(), plot_type="partial")+
  ggtitle("Residuals of differenced RPCE: Non-Durable Goods")
```


* There is no spike seen in this ACF and PACF, so lets run auto arima model
* Lets also check ETS models: ETS(A,N,A) and ETS(A,N,A),ETS(M,N,M) and ETS()


```{r}
#test data set
nrpce_test <- rpce_ndurable_goods_gdp %>% 
  filter_index("2020 Q1" ~.)

#training data set
nrpce_train <- rpce_ndurable_goods_gdp %>% 
  filter_index(~ "2019 Q4")

arima_models1 <- nrpce_train %>%
  model(auto_arima = ARIMA(Expenditure, stepwise = FALSE, approx = FALSE)
  )

glance(arima_models1) %>% arrange(AICc)

ets_models1 <- nrpce_train %>%
  model(
    ANA = ETS(Expenditure ~ error("A") + trend("N") + season("A")),
    MNM = ETS(Expenditure ~ error("M") +  trend("N") + season("M")),
    
    
    ETS_auto = ETS(Expenditure)
    )
accuracy(ets_models1)

```


```{r}
best_arima_model1_nd <- arima_models1 %>% select(auto_arima)
report(best_arima_model1_nd)
```



**Best ARIMA model-RPCE: ARIMA(1,0,0)(1,1,0)[4]**

```{r}
#looking at residuals
best_arima_model1_nd %>%
  gg_tsresiduals()+ ggtitle("Residual analysis of RPCE Arima model")

#Doing Ljung box test on residuals
Box.test(augment(best_arima_model1_nd)$.resid, lag=36, fitdf = 2, type = "Lj")
```

* **Observations**

  * By looking at distribution of the residuals, it looks like slightly skewed
  * There is also a constant variance seen in the time plot of residuals
  * The ACF plot shows no auto-correlation which is confirmed by performing Ljung Box test.        Here p-value >0.05, we cannot reject null hypothesis, and say that there is no                 autocorrelation and this is like white noise


**Best ETS model-PRCE: ETS_auto**

```{r}
nbest_ets_model1 <- ets_models1 %>%
  select(ETS_auto) 
report(nbest_ets_model1)

nbest_ets_model1 %>%
  gg_tsresiduals(lag_max = 24)+ggtitle("Residual analysis of RPCE Non Durable: ETS model")

```



* **Observations**

  * The histograms of the residual is normal and centered around zero, which indicates that        probably the forecast from this method will be good, and also prediction intervals             computed will be accurate
  * The time plot of the residuals shows that the variation of the residuals stays                 approximately the same across the historical data, apart from some outliers
  * The residuals looks like white noise as there is no auto correlation seen
  
* **Selecting best model for RPCE: Durable goods(ARIMA or ETS)**
```{r}
# Generate forecasts and compare accuracy over the test set
bind_rows(
    best_arima_model1_nd %>% accuracy(),
    nbest_ets_model1 %>% accuracy(),
    best_arima_model1_nd %>% forecast(h = "4 years") %>% accuracy(nrpce_test),
    nbest_ets_model1 %>% forecast(h = "4 years") %>% accuracy(nrpce_test)
  ) %>%
  select(-ME, -MPE, -ACF1)
```

 
**So, since RMSE of ETS(M,N,M) comes out to be lowest on test data. Hence, ETS(M,N,M) is a better model. We will forecast the real personal consumption expenditure as % of GDP for 1 year ahead by using ETS(M,N,M) model**

<br>

**4.2.Industrial Production: Non-Durable Goods**

<br>
**Looking at time series**
```{r}
#plotting graphs to see our time series
g1<- industrial_prod_ndurable_goods %>%
  autoplot(Index) + ggtitle("Industrial Production: Non- Durable Goods") + ylab("Index")
g2<- industrial_prod_ndurable_goods %>%
  gg_season(Index) + ggtitle("Industrial Production: Non - Durable Goods") + ylab("Index")
grid.arrange(g1, g2, nrow = 2)
```



```{r}
#Checking for different components in the series
industrial_prod_ndurable_goods %>%
  model(
    STL(Index)) %>%
  components() %>%
  autoplot()+
  labs(title = "STL decomposition of Industrial Production: Non-Durable goods")
```


* So, there is a strong seasonality in the series which is increasing over time

**Forecast: Industrial production**

```{r}
#transform the data and stabilize as there is variation in the series which is increasing with time

industrial_prod_ndurable_goods %>%
  autoplot(log(Index)) +
  labs(y = "", title = "Log Transformed Industrial Production for Non- Durable Goods")

#Step 2: Checking for stationary
industrial_prod_ndurable_goods %>% ACF(Index) %>% autoplot()+ ggtitle("ACF plot for Industrial production: Non- Durable goods")


#Running KPSS to statistically justify that data is stationary or not and find order of the differincing - Null hypothesis (Ho): Data is stationary - Alternate hypothesis (Halt): Data is non-stationary

industrial_prod_ndurable_goods %>% features(log(Index), unitroot_kpss)
```



* The time series is non-stationary as it clearly has trend and seasonality in it, which we observed in above graphs
* The ACF plot is decaying very slowly, which is indicative of non-stationarity in the time series
* Also, the p-value <0.05 in KPSS test, so we reject null hypothesis of KPSS test and can confirm that data is not stationary
* So, we require to see differenced time series

```{r}
#no. of seasonal difference - industrial production
industrial_prod_ndurable_goods %>% mutate(t_Index = log(Index)) %>%
  features(t_Index, unitroot_nsdiffs)

#no. of regular difference - industrial production
industrial_prod_ndurable_goods %>% mutate(d_Index = difference(log(Index),12)) %>% features(d_Index, unitroot_ndiffs)
```

* So, we need just one seasonal difference and one normal to make this data stationary as per above test results

```{r}
industrial_prod_ndurable_goods %>%
  autoplot(log(Index)%>% difference(lag=12)%>%difference())+ 
  ggtitle("Industrial Production: Non- Durable Goods(Differenced timeseries)")+ylab("Index")

#KPSS on box-cox transformed differenced time series
industrial_prod_ndurable_goods %>% mutate(d_Index = difference(log(Index),12)%>%difference()) %>%  features(d_Index, unitroot_kpss)
```


* The seasonally differenced time series looks now to be stationary
* KPSS test also confirm our finding as the p value from above test is >0.05 and so we cannot     reject null hypothesis and say that this differenced data is now stationary

```{r}
#checking residuals of differenced time series
industrial_prod_ndurable_goods %>%
  gg_tsdisplay(log(Index) %>% difference(lag=12)%>% difference(), plot_type="partial")+
  ggtitle("Residuals of differenced Industrial Production: Non- Durable Goods")
```

**ACF**

* Spike is seen at lag 12 and lag 24, which is seasonal MA(1)or MA(2) component
* Spike at lag 1 indicates non-seasonal MA(1) component

**PACF**

* We can see negative spike at lag 12 and lag42. This is seasonal spikes. Further these seasonal spike seems to be decaying exponentially over time. This suggest AR(1) or AR(2) seasonal component
* Significant partial autocorrelation is seen at lag 1. This is non- seasonal spike which suggest AR(1) non-seasonal component

* Moodels suggested : 
  * Model 1: ARIMA(1,1,0)(1,1,0)12
  * Model 2: ARIMA(1,1,0)(2,1,0)12
  * Model 2: ARIMA(1,1,0)(0,1,1)12
  * Model 2: ARIMA(0,1,1)(0,1,2)12


```{r}
#test data set
nip_test <- industrial_prod_ndurable_goods %>% 
  filter_index("2019 Jan" ~.)

#training data
nip_train <- industrial_prod_ndurable_goods %>% 
  filter_index(~ "2018 Dec")

arima_models2 <- nip_train %>%
  model(
    arima110110 = ARIMA(log(Index) ~ pdq(1,1,0) + PDQ(1,1,0)),
    arima110210 = ARIMA(log(Index) ~ pdq(1,1,0) + PDQ(2,1,0)),
     arima110011 = ARIMA(log(Index) ~ pdq(1,1,0) + PDQ(0,1,1)),
    arima011012 = ARIMA(log(Index) ~ pdq(0,1,1) + PDQ(0,1,2)),
    
    
    auto_arima1 = ARIMA(log(Index), stepwise = FALSE, approx = FALSE)
  )

glance(arima_models2) %>% arrange(AICc)
```


* The lowest AICc is of auto_arima1 model which is a little better than our guessed model arima011012
* So, we will take auto_arima1 model for our forecast

```{r}
nbest_arima_model2 <- arima_models2 %>% select(auto_arima1)
report(nbest_arima_model2)
```



**Best ARIMA model-Industrial Production: ARIMA(1,1,2)(0,1,1)[12]  **

```{r}
#looking at residuals
nbest_arima_model2 %>%
  gg_tsresiduals()+ ggtitle("Residual analysis of Industrial Production Arima model")

#Doing Ljung box test on residuals
Box.test(augment(nbest_arima_model2)$.resid, lag=36, fitdf = 4, type = "Lj")
```


* **Observations**

  * By looking at distribution of the residuals, it looks like normal
  * There is also a constant variance seen in the time plot of residuals
  * The ACF plot shows no auto-correlation but we confirmed by performing Ljung Box test. Here      p-value <0.05, we  reject null hypothesis, and say that there is  autocorrelation       and residuals do not look like white noise

<br>

* **Using ETS Model**

  * Since, the industrial production for non durable goods time series is seasonal and has trend     in it, we think it should be ETS(A,Ad,A)
  * Since seasonality is increasing over time, it is good to see multiplicative models as well     :ETS(M,Ad,M) model
  * Let's test the possible models

```{r}
ets_models2 <- nip_train %>%
  model(
    AAdA = ETS(log(Index) ~ error("A") + trend("Ad") + season("A")),
    AAA = ETS(log(Index) ~ error("A") +  trend("A") + season("A")),
    MAM = ETS(log(Index) ~ error("M") +  trend("A") + season("M")),
    MAdM = ETS(log(Index) ~ error("M") +  trend("Ad") + season("M")),
    
    ETS = ETS(log(Index))
    )
accuracy(ets_models2)
``` 


**Best ETS Model- Industrial Production: ETS(M,A,A)**

```{r}
nbest_ets_model2 <- ets_models2 %>%
  select(ETS) 
report(nbest_ets_model2)
nbest_ets_model2 %>%
  gg_tsresiduals(lag_max = 24)+ggtitle("Residual analysis of Industrial production Non Durable: ETS model")

#Doing Ljung box test on residuals
Box.test(augment(nbest_ets_model2)$.resid, lag=36, fitdf = 6, type = "Lj")

```



* **Observations**

  * By looking at distribution of the residuals, it looks like normal
  * There is also a constant variance seen in the time plot of residuals
  * The ACF plot shows  auto-correlation which is confirmed by performing Ljung Box test. Here      p-value <0.05, we reject null hypothesis, and say that there is autocorrelation and             residuals do not look like white noise


* **Selecting best model for Industrial Production: Non-Durable goods(ARIMA or ETS)**

```{r}
# Generate forecasts and compare accuracy over the test set
bind_rows(
    nbest_arima_model2 %>% accuracy(),
    nbest_ets_model2 %>% accuracy(),
  
    nbest_arima_model2 %>% forecast(h = "4 years") %>% accuracy(nip_test),
    nbest_ets_model2%>% forecast(h = "4 years") %>% accuracy(nip_test)
  ) %>%
  select(-ME, -MPE, -ACF1)
```


**So, since RMSE of ETS(M,A,A) comes out to be lowest on test data. Hence, ETS(M,A,A) is a better model. We will forecast the industrial production of non durable goods for 1 year ahead by using ETS(M,A,A) model**


<br>

**4.3.Producer Price Index by Commodity: Non-Durable Goods**

<br>

**Looking at time series**
```{r}
#plotting graphs to see our time series
g1<- PPI_ndurable_goods %>%
  autoplot(Index) + ggtitle("Producer Price Index: Non- Durable Goods") + ylab("Index")
g2<- PPI_ndurable_goods %>%
  gg_season(Index) + ggtitle("Producer Price Index: Non- Durable Goods") + ylab("Index")
grid.arrange(g1, g2, nrow = 2)
```



```{r}
#Checking for different components in the series
PPI_ndurable_goods %>%
  model(
    STL(Index)) %>%
  components() %>%
  autoplot()+
  labs(title = "STL decomposition of PPI: Non Durable goods")
```



* So we could see an upward trend and seasonality changing with time in the series but its very low seasonality

**Forecast: PPI-Non Durable goods**

```{r}
#transform the data and stabilize as there is variation in the series which is increasing with time

PPI_ndurable_goods %>%
  autoplot(log(Index)) +
  labs(y = "", title = "Log Transformed PPI By Commodity:Non Durable Goods")

#Step 2: Checking for stationary
PPI_ndurable_goods %>% ACF(Index) %>% autoplot()+ggtitle("ACF plot for PPI: Non Durable goods")

#Running KPSS to statistically justify that data is stationary or not and find order of the differincing - Null hypothesis (Ho): Data is stationary - Alternate hypothesis (Halt): Data is non-stationary
PPI_ndurable_goods %>% features(log(Index), unitroot_kpss)
```



* The time series is non-stationary as it clearly has trend in it, which we observed in above graphs
* The ACF plot is decaying very slowly, which is indicative of non-stationarity in the time series
* Also, the p-value <0.05 in KPSS test, so we reject null hypothesis of KPSS test and can confirm that data is not stationary
* So, we require to see differenced time series


```{r}
#no. of seasonal difference - PPI
PPI_ndurable_goods %>% mutate(t_Index = log(Index)) %>%
  features(t_Index, unitroot_nsdiffs)

#no. of regular difference - PPI
PPI_ndurable_goods %>%  features(log(Index), unitroot_ndiffs)
```


* So, we need just one normal difference to make this data stationary as per above test results


```{r}
PPI_ndurable_goods %>%
  autoplot(log(Index)%>% difference())+ 
  ggtitle("PPI: Non Durable Goods(Differenced timeseries)")+ylab("Index")

#KPSS on box-cox transformed differenced time series
PPI_ndurable_goods %>% mutate(d_Index = (difference(log(Index)))) %>%  features(d_Index, unitroot_kpss)
```

* The  differenced time series looks now to be stationary
* KPSS test also confirm our finding as the p value from above test is >0.05 and so we cannot reject null hypothesis and say that this differenced data is now stationary

```{r}
#checking residuals of differenced time series
PPI_ndurable_goods %>%
  gg_tsdisplay(log(Index) %>% difference() , plot_type="partial")+
  ggtitle("Residuals of differenced PPI: Non Durable Goods")
```




**ACF**

* There is a significant negative spike at lag 1, which suggest non-seasonal MA(1) component


**PACF**

* The lags can be seen to be decaying exponentially. There is some negative partial autocorrelation at lag1 significant, which suggests AR(1) non-seasonal components

* Suggested Model
  * ARIMA(0,1,1)
  * ARIMA(1,1,0)

  

```{r}
#test data set
nppi_test <- PPI_ndurable_goods %>% 
  filter_index("2019 Jan" ~.)

nppi_train <- PPI_ndurable_goods %>% 
  filter_index(~ "2018 Dec")

arima_models3 <- nppi_train %>%
  model(
    arima011000 = ARIMA(log(Index) ~ pdq(0,1,1) + PDQ(0,0,0)),
    arima110000 = ARIMA(log(Index) ~ pdq(1,1,0) + PDQ(0,0,0)),
  

    
    arima_model = ARIMA(log(Index), stepwise = FALSE, approx = FALSE)
    
  )
glance(arima_models3) %>% arrange(AICc)
```



* The lowest AICc is of auto arima_model model, which is very close to one of our guess arima110000
* So, we will use arima_model for our forecast as best model


```{r}
nbest_arima_model3 <- arima_models3 %>% select(arima_model)
report(nbest_arima_model3)
```



**Best ARIMA model-PPI: ARIMA(2,1,1)(0,0,2)[12] w/ drift  **

```{r}
#looking at residuals
nbest_arima_model3 %>%
  gg_tsresiduals()+ ggtitle("Residual analysis of PPI Arima model")

#Doing Ljung box test on residuals
Box.test(augment(nbest_arima_model3)$.resid, lag=36, fitdf = 5, type = "Lj")
```


* **Observations**

  * By looking at distribution of the residuals, it looks like normal
  * There is not a constant variance seen in the time plot of residuals
  * The ACF plot shows a little auto-correlation which is confirmed by performing Ljung Box        test. Here p-value <0.05, we reject null hypothesis, and say that there is autocorrelation     and residuals do not look like white noise

<br>

* **Using ETS Model**

```{r}
st <- decomposition_model(
  STL(log(Index)),
  ETS(season_adjust ~ error("A") + trend("Ad") + season("N"))
)
ets_models3 <- nppi_train %>%
  model(
    snaive = SNAIVE(log(Index)),
    AAdN = ETS(log(Index) ~ error("A") + trend("Ad") + season("N")),
    AAN = ETS(log(Index) ~ error("A") +  trend("A") + season("N")),
    MAN = ETS(log(Index) ~ error("M") +  trend("A") + season("N")),
    MAdN = ETS(log(Index) ~ error("M") +  trend("Ad") + season("N")),
    ETS = ETS(log(Index)),
    
    stl_ets = st
    )
accuracy(ets_models3)
```




**Best ETS Model- PPI Non Durable goods:stl_ets**

```{r}
nbest_ets_model3 <- ets_models3 %>%
  select(stl_ets)
report(nbest_ets_model3)
nbest_ets_model3 %>%
  gg_tsresiduals(lag_max = 24)+ggtitle("Residual analysis of PPI Durable: ETS model")

#Doing Ljung box test on residuals
Box.test(augment(nbest_ets_model3)$.resid, lag=36, fitdf = 5, type = "Lj")
```

* **Observations**

  * By looking at distribution of the residuals, it looks like normal
  * There is not constant variance seen in the time plot of residuals
  * The ACF plot shows auto-correlation which is confirmed by performing Ljung Box test. Here       p-value <0.05, we reject null hypothesis, and say that there is autocorrelation and             residuals do not look like white noise

* **Selecting best model for PPI: Durable goods(ARIMA or ETS)**

```{r}
# Generate forecasts and compare accuracy over the test set
bind_rows(
    nbest_arima_model3 %>% accuracy(),
    nbest_ets_model3 %>% accuracy(),
    nbest_arima_model3 %>% forecast(h = "4 years") %>% accuracy(nppi_test),
    nbest_ets_model3 %>% forecast(h = "4 years") %>% accuracy(nppi_test)
  ) %>%
  select(-ME, -MPE, -ACF1)
```


**Clearly, ARIMA model performed better than ETS on test data here as RMSE of arima model is lesser. So we will use ARIMA model to forecast PPI**


**4.4.Consumer Price Index for all Urban Consumers: Non- Durable Goods**

<br>
**Looking at time series**
```{r}
#plotting graphs to see our time series
g1<- CPI_ndurable_goods %>%
  autoplot(Index) + ggtitle("Consumer Price Index: Non-Durable Goods")
g2<- CPI_ndurable_goods %>%
  gg_season(Index) + ggtitle("Consumer Price Index: Non-Durable Goods")
grid.arrange(g1, g2, nrow = 2)
```



```{r}
#Checking for different components in the series
CPI_ndurable_goods %>%
  model(
    STL(Index)) %>%
  components() %>%
  autoplot()+
  labs(title = "STL decomposition of CPI: Non-Durable goods")
```


**Forecast: CPI-non durable goods**

```{r}
#transform the data and stabilize as there is variation in the series which is increasing with time
CPI_ndurable_goods %>%
  autoplot(log(Index)) +
  labs(y = "", title = "Log Transformed Consumer Price Index for Non Durable Goods")

#Step 2: Checking for stationary
CPI_ndurable_goods %>% ACF(Index) %>% autoplot()+ggtitle("ACF plot for CPI: Non Durable goods")

#Running KPSS to statistically justify that data is stationary or not and find order of the differincing - Null hypothesis (Ho): Data is stationary - Alternate hypothesis (Halt): Data is non-stationary
CPI_ndurable_goods %>% features(log(Index), unitroot_kpss)
```


* The time series is non-stationary as it clearly has trend in it, which we observed in above graphs
* The ACF plot is decaying very slowly, which is indicative of non-stationarity in the time series
* Also, the p-value <0.05 in KPSS test, so we reject null hypothesis of KPSS test and can confirm that data is not stationary
* So, we require to see differenced time series


```{r}
#no. of seasonal difference - Consumer price index
CPI_ndurable_goods %>% mutate(t_Index = log(Index)) %>%
  features(t_Index, unitroot_nsdiffs)
#no. of regular difference - industrial production
CPI_ndurable_goods %>%  features(log(Index), unitroot_ndiffs)
```

* So, we need just two normal difference to make this data stationary as per above test results

```{r}
CPI_ndurable_goods %>%
  autoplot(log(Index)%>% difference() %>% difference())+ 
  ggtitle("CPI: Durable Goods(Differenced timeseries)")+ylab("Index")

#KPSS on box-cox transformed differenced time series
CPI_ndurable_goods %>% mutate(d_Index = difference(difference(log(Index)))) %>%  features(d_Index, unitroot_kpss)
```


* The double differenced time series looks now to be stationary
* KPSS test also confirm our finding as the p value from above test is >0.05 and so we cannot reject null hypothesis and say that this differenced data is now stationary


```{r}
#checking residuals of differenced time series
CPI_ndurable_goods %>%
  gg_tsdisplay(log(Index) %>% difference() %>% difference(), plot_type="partial")+ 
  ggtitle("Residuals of differenced CPI: Non Durable Goods")
```
**ACF**

* There is a significant negative spike at lag 1 and lag 2, which suggest MA(1) or MA(2)component 

**PACF**

* The lags can be seen to be decaying exponentially.


* Suggested Model: ARIMA(0,2,1) or ARIMA(1,2,1) or ARIMA(0,2,2)


```{r}
#test data set
cpi_ntest <- CPI_ndurable_goods %>% 
  filter_index("2019 Jan" ~.)

cpi_ntrain <- CPI_ndurable_goods %>% 
  filter_index(~ "2018 Dec")

arima_all_models11 <- cpi_ntrain %>%
  model(
    arima021 = ARIMA(log(Index) ~ pdq(0,2,1) + PDQ(0,0,0)),
    arima121 = ARIMA(log(Index) ~ pdq(1,2,1) + PDQ(0,0,0)),
    arima120 = ARIMA(log(Index) ~ pdq(1,2,0) + PDQ(0,0,0)),
    arima022 = ARIMA(log(Index) ~ pdq(0,2,2) + PDQ(0,0,0)),
   arima_model = ARIMA(log(Index), stepwise = FALSE, approx = FALSE)
    
  )
glance(arima_all_models11) %>% arrange(AICc)

```


* The lowest AICc is of auto arima_model model

```{r}
best_arima_model11 <- arima_all_models11 %>% select(arima_model)
report(best_arima_model11)
```

**Best ARIMA model-CPI: ARIMA(0,2,4)(0,0,2)[12]**

```{r}
#looking at residuals
best_arima_model11 %>%
  gg_tsresiduals()+ ggtitle("Residual analysis of CPI Arima model")
#Doing Ljung box test on residuals
Box.test(augment(best_arima_model11)$.resid, lag=36, fitdf = 6, type = "Lj")
```


* **Observations**

  * By looking at distribution of the residuals, it looks like normal 
  * There is not a constant variance seen in the time plot of residuals 
  * The ACF plot shows some auto-correlation which is confirmed by performing Ljung Box test.       Here p-value <0.05, we reject null hypothesis, and say that there is autocorrelation and        residuals do not look like white noise

* **Using ETS model**

  * Since, the CPI for durable goods time series has trend in it, we think it should be             ETS(A,A,N), ETS(A,Ad,N), ETS(M,Ad,N) or ETS(M,A,N) model. Let’s test the possible models


```{r}
stl2 <- decomposition_model(
  STL(log(Index)),
  ETS(season_adjust ~ error("A") + trend("Ad") + season("N"))
)

ets_all_models11 <- cpi_ntrain %>%
  model(AAdN = ETS(log(Index) ~ error("A") + trend("Ad") + season("N")),
    AAN = ETS(log(Index) ~ error("A") +  trend("A") + season("N")),
    MAN = ETS(log(Index) ~ error("M") +  trend("A") + season("N")),
    MAdN = ETS(log(Index) ~ error("M") +  trend("Ad") + season("N")),
    
    stl_ets = stl2,
    ETS = ETS(log(Index))
    )
accuracy(ets_all_models11)
```
* The STL decomposition forecasts using the additive trend model, ETS(A,Ad,N), is slightly better in-sample. However, note that this is a biased comparison as the models have different numbers of parameters. So, we will test for the accuracy of both the ets models on test data and select the best one.


**Best ETS Model-CPI: Durable goods: ETS(A,Ad,N) and stl decomposition ETS(A,Ad,N)**

```{r}
best_ets_model11_1 <- ets_all_models11 %>%
  select(AAdN)
best_ets_model11_2 <- ets_all_models11 %>%
  select(stl_ets)
best_ets_model11_1 %>%
  gg_tsresiduals(lag_max = 24)+ggtitle("Residual analysis of CPI Non-Durable: ETS model")
#Doing Ljung box test on residuals
Box.test(augment(best_ets_model11_1)$.resid, lag=36, fitdf = 5, type = "Lj")

best_ets_model11_2 %>%
  gg_tsresiduals(lag_max = 24)+ggtitle("Residual analysis of CPI Non-Durable: ETS model")
#Doing Ljung box test on residuals
Box.test(augment(best_ets_model11_2)$.resid, lag=36, fitdf = 5, type = "Lj")
```

* **Observations**

  * By looking at distribution of the residuals, it looks like normal
  * There is not constant variance seen in the time plot of residuals
  * The ACF plot shows auto-correlation which is confirmed by performing Ljung Box test. Here       p-value <0.05, we reject null hypothesis, and say that there is autocorrelation and             residuals do not look like white noise
  
* **Plotting both forecasts**

```{r}
fc15 <- best_arima_model11 %>% forecast(h = "5 years")
fc16_1 <- best_ets_model11_1 %>% forecast(h = "5 years")
fc16_2 <- best_ets_model11_2 %>% forecast(h = "5 years")
c1<- fc15 %>%
  autoplot(cpi_ntrain ) +
  geom_line(data=cpi_ntest, aes(x=Month, y=Index), col='red')+
  ggtitle("CPI: Non-Durable Goods: ARIMA")+ylab("Index")
c2<- fc16_1 %>%
  autoplot(cpi_ntrain) +
  geom_line(data=cpi_ntest, aes(x=Month, y=Index), col='red')+
  ggtitle("CPI: Non-Durable Goods: ETS")+ylab("Index")
c3<- fc16_2 %>%
  autoplot(cpi_ntrain) +
  geom_line(data=cpi_ntest, aes(x=Month, y=Index), col='red')+
  ggtitle("CPI: Non-Durable Goods: ETS")+ylab("Index")
grid.arrange(c1,c2,c3, nrow =3)
```

* **Selecting best model for CPI: Durable goods(ARIMA or ETS)**

```{r}
# Generate forecasts and compare accuracy over the test set
bind_rows(
    best_arima_model11 %>% accuracy(),
    best_ets_model11_1 %>% accuracy(),
    best_ets_model11_2 %>% accuracy(),
    best_arima_model11 %>% forecast(h = "3 years") %>% accuracy(cpi_ntest),
    best_ets_model11_1 %>% forecast(h = "3 years") %>% accuracy(cpi_ntest),
    best_ets_model11_2 %>% forecast(h = "3 years") %>% accuracy(cpi_ntest)
  ) %>%
  select(-ME, -MPE, -ACF1)
```

**Clearly, arima_model model performed better than ETS on test data here as RMSE of ARIMA model is lesser. So we will use ARIMA model to forecast CPI**

<br>

### 5.Services-Modelling


**5.1.Real Personal Consumption Expenditure**

**Looking at the series**

```{r}
data_rpce_services_gdp %>% autoplot(.vars=Expenditure) + labs(title = "Real Personal Consumption Expenditure:Services") +
  theme(plot.title = element_text(hjust = 0.5))
```
data_rpce_services_gdp

```{r}

#no. of seasonal difference
data_rpce_services_gdp %>% mutate(t_Expenditure = log(Expenditure)) %>%
  features(t_Expenditure, unitroot_nsdiffs)
#no. of regular difference
data_rpce_services_gdp %>% mutate(d_Expenditure = difference(log(Expenditure),4)) %>% features(d_Expenditure, unitroot_ndiffs)
```


* So, we need just one seasonal difference to make this data stationary as per above test results


```{r}
data_rpce_services_gdp %>%
  autoplot((Expenditure)%>% difference(lag=4))+ 
  ggtitle("Real personal consumption Expenditure:Services(Differenced timeseries)")+ylab("% of GDP")

#KPSS on box-cox transformed seasonal differenced time series
data_rpce_services_gdp %>% mutate(t_Expenditure = difference((Expenditure),4)) %>%
  features(t_Expenditure, unitroot_kpss)
```


* The seasonally differenced time series looks now to be stationary
* KPSS test also confirm our finding as the p value from above test is >0.05 and so we cannot reject null hypothesis and say that this differenced data is now stationary

**ACF plot**

* We can see positive spikes at lag 1.This suggests of MA(1) non-seasonal component

**PACF plot**

* We can see a  spike at lag 1, which suggest AR(1) non-seasonal component 
  * Model 1: ARIMA(1,1,0)
  * Model 2: ARIMA(0,0,1)
  

```{r}
data_rpce_services_gdp_fit <- data_rpce_services_gdp %>%
  model(arima110 = ARIMA(Expenditure ~ pdq(1,1,0)),
        arima010 = ARIMA(Expenditure ~ pdq(0,1,0)),
        stepwise = ARIMA(Expenditure),
        auto = ARIMA(Expenditure, stepwise=FALSE))
report (data_rpce_services_gdp_fit)
```

```{r}
rcpe_services_fit <- data_rpce_services_gdp %>%
  model(ARIMA((Expenditure))) %>%
  report(fit)
```

**Best model - ARIMA(1,0,0)(0,1,1)[4]** 

```{r}
rcpe_services_fit %>% gg_tsresiduals()

#Doing Ljung box test on residuals
Box.test(augment(rcpe_services_fit)$.resid, lag=36, fitdf = 2, type = "Lj")
```

```{r}
final_arima <-data_rpce_services_gdp %>% 
  model(ARIMA((Expenditure)))

final_arima %>%
  forecast(h = "1 years") %>%
  autoplot(data_rpce_services_gdp, level = NULL) + labs(title = "") +
  theme(plot.title = element_text(hjust = 0.5))
```


* **Observations**

  * By looking at distribution of the residuals, it looks like slightly skewed
  * There is also a constant variance seen in the time plot of residuals
  * The ACF plot shows no auto-correlation which is confirmed by performing Ljung Box test.        Here p-value >0.05, we cannot reject null hypothesis, and say that there is no                 autocorrelation and this is like white noise



**5.2.Real Net Exports of Goods and Services**

```{r}
data_netexp_gdp %>% autoplot(.vars=Expenditure) + labs(title = "Net Exports of Goods and Services") +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
#no. of seasonal difference
data_netexp_gdp %>% mutate(t_Expenditure = (Expenditure)) %>%
  features(t_Expenditure, unitroot_nsdiffs)
#no. of regular difference
data_netexp_gdp %>% mutate(d_Expenditure = difference(Expenditure,4)) %>% features(d_Expenditure, unitroot_ndiffs)
```
* So, we need just one seasonal and one normal difference to make this data stationary as per above test results


```{r}
data_netexp_gdp %>%
  autoplot(Expenditure%>% difference(lag=4)%>% difference())+ 
  ggtitle("Net Exports as % of GDP: Services(Differenced timeseries)")+ylab("% of GDP")

#KPSS on transformed seasonal differenced time series
data_netexp_gdp %>% mutate(t_Expenditure = difference(difference(Expenditure,4)))%>%
  features(t_Expenditure, unitroot_kpss)
```

* The double differenced time series looks now to be stationary
* KPSS test also confirm our finding as the p value from above test is >0.05 and so we cannot reject null hypothesis and say that this differenced data is now stationary

```{r}
#checking residuals of differenced time series
data_netexp_gdp %>%
  gg_tsdisplay(Expenditure %>% difference(lag=4)%>%difference(), plot_type="partial")+
  ggtitle("Residuals of differenced Net Exports: Services")
```



```{r}
data_netexp_gdp_fit <- data_netexp_gdp %>%
  model(arima010 = ARIMA(Expenditure ~ pdq(0,1,0)),
        arima012 = ARIMA(Expenditure ~ pdq(0,1,2)),
        stepwise = ARIMA(Expenditure),
        auto = ARIMA(Expenditure, stepwise=FALSE))
report (data_netexp_gdp_fit)
```



```{r}
netexp_gdp_fit <- data_netexp_gdp_fit%>%select(auto) %>%
  report(netexp_gdp_fit)
```


**Best ARIMA model-RPCE: ARIMA(1,1,5)(0,1,0)[4]**


```{r}
netexp_gdp_fit %>% gg_tsresiduals()
#Doing Ljung box test on residuals
Box.test(augment(netexp_gdp_fit)$.resid, lag=36, fitdf = 6, type = "Lj")
```

* **Observations**

  * By looking at distribution of the residuals, it looks like slightly skewed
  * There is also a constant variance seen in the time plot of residuals
  * The ACF plot shows no auto-correlation which is confirmed by performing Ljung Box test.        Here p-value >0.05, we cannot reject null hypothesis, and say that there is no                 autocorrelation and this is like white noise



```{r}
netexp_gdp_fit %>%
  forecast(h = "1 year") %>%
  autoplot(data_netexp_gdp, level = NULL) + labs(title = "") +
  theme(plot.title = element_text(hjust = 0.5))
```


**5.3.All Employees, Service-Providing**
```{r}
data_ae_sp_1 <- data_ae_sp %>%mutate(Month = yearmonth(DATE)) %>%
as_tsibble(index=Month)
```


```{r}
data_ae_sp_1 %>% autoplot(.vars=SRVPRD) + labs(title = "All Employees, Service-Providing") +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
#no. of seasonal difference
data_ae_sp_1 %>% mutate(t_SRVPRD = SRVPRD) %>%
  features(t_SRVPRD, unitroot_nsdiffs)
#no. of regular difference
data_ae_sp_1 %>% mutate(t_SRVPRD = difference(SRVPRD,12)) %>% features(t_SRVPRD, unitroot_ndiffs)
```


* No differencing is needed

```{r}
data_ae_sp_1 %>%
  gg_tsdisplay(SRVPRD, plot_type='partial')
```



```{r}
data_ae_sp_fit <- data_ae_sp_1 %>%
  model(arima111 = ARIMA(SRVPRD ~ pdq(1,1,1)),
        arima012 = ARIMA(SRVPRD ~ pdq(0,1,2)),
        stepwise = ARIMA(SRVPRD),
        auto = ARIMA(SRVPRD, stepwise=FALSE))
report (data_ae_sp_fit)
```


```{r}
ae_sp_fit <- data_ae_sp %>%
  model(ARIMA(SRVPRD)) %>%
  report(ae_sp_fit)
```

**Best Model: ARIMA(0,1,2) w/ drift**

```{r}
ae_sp_fit %>% gg_tsresiduals()
```

```{r}
ae_sp_final_arima <-data_ae_sp %>% 
  model(ARIMA(SRVPRD))

ae_sp_final_arima %>%
  forecast(h = "1 years") %>%
  autoplot(data_ae_sp, level = NULL) + labs(title = "") +
  theme(plot.title = element_text(hjust = 0.5))
```
* **Observations**

  * By looking at distribution of the residuals, it looks like slightly skewed
  * There is also a constant variance seen in the time plot of residuals
  * The ACF plot shows no auto-correlation which is confirmed by performing Ljung Box test.        Here p-value >0.05, we cannot reject null hypothesis, and say that there is no                 autocorrelation and this is like white noise


<br>

### 6.Economic Outlook Report

```{r}
theme_set(theme_bw()) # this is a base theme I like

theme_update(axis.title = element_text(size = 11), axis.text = element_text(size = 10),
             legend.text = element_text(size = 10), legend.title = element_text(size = 10), 
             panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

**Real Personal Consumption Expenditure: Durable Goods**

* Real personal consumption expenditures (PCE) is the primary measure of consumer spending on goods and services in the U.S. economy
* Durable goods have an average useful life of at least 3 years (e.g. motor vehicles) 
* The chart below shows what percentage of GDP is driven by (inflation-adjusted) personal consumption expenditures on Durable Goods
* Between 2005 and 2021, there is a clear upward trend, signifying that consumers are spending a larger percentage of their real income on durable goods. Moreover, our forecast shows a continuation of the upward trend
* Consumer demand for durable goods has increased substantially in the last decade, and shows signs of continuing on this trajectory in the future
* 2020 and 2021 seems to be following a different trend. 2020 has seen a slight drop in expenditure in Q1 but after that the personal consumption expenditure starts forming a major part of GDP and the series seems to be rising sharply. This is right after covid lockdown started and was clear to people that it is going to stay for long. So, as expected people started spending more in durable goods. Lockdowns and social distancing shifted consumer demand away from services toward durable goods
* Personal Consumption Expenditure for Durable Goods in the United States is expected to be 12.08 % of GDP by the end of this quarter. In the long-term, the United States Personal Consumption Expenditure for Durable goods is projected to trend around 11.9% in 2023 Q1, according to our econometric models


```{r, dpi=300}
#Since best model for rpce was ETS(AAA) when we compared different models, so using it
rpce_model <- rpce_durable_goods_gdp %>%
  model(AAA = ETS(box_cox(Expenditure,lambda1) ~ error("A") +  trend("A") + season("A")))
forecast1 <- rpce_model %>% forecast(h = "1 years")
f1<- forecast1 %>% autoplot(rpce_durable_goods_gdp) +
  labs(y = "Expenditure(%of GDP)", title = "RPCE: Durable Goods")

#Plotting Real disposable income as well
rdpi_model<- rdpi %>%
  model(AAN = ETS(Income ~ error("A") +  trend("A") + season("N")))
fc <- rdpi_model %>% forecast(h = "1 years")
f2<- fc %>% autoplot(rdpi %>% filter(year(Quarter)>2000)) +
  labs(y = "Income", title = "Real Personal Disposable Income")
grid.arrange(f1,f2,nrow=2)
```


* Real Personal Disposable Income has been on a consistent upward trend for the last two decades, despite having three domestic recessions during that period
* The trend line shows no signs of this stopping or slowing down
* It is also possible that increase in disposable income resulting from fiscal policy measures stimulated consumption expenditures, including those on durable goods 
* The increase in disposable income largely reflects the forceful fiscal policy measures that were put in place to deal with the economic fallout of the pandemic
* Understanding the factors behind the rise in durable goods spending can inform policymakers’ assessments of the pandemic policy responses. On the one hand, substitution of durable goods for services would indicate that the overall economic cost of health policy measures that restrict services is smaller than indicated by the observed decline in services consumption alone. On the other hand, a boost to consumer spending from higher disposable incomes would corroborate the efficacy of fiscal stimulus
* When juxtaposing Real Personal Consumption Expenditure on Durable Goods and Real Personal Disposable Income, two takeaways are clear
  * US Consumers have an increasing appetite for durable goods
  * US Consumers have more real dollars in their pocket to spend on non-essential or               less-essential products that will last them for 3+ years


<br>

**Industrial Production/PPI and PPI/CPI: Durable Goods**

* Whereas Real Personal Consumption Expenditure represents consumer demand for Durable Goods, Industrial Production represents the supply of these products
* Looking at the red line in the first chart below, we can see that the industrially produced supply of durable goods has kept pace with demand. In other words, the two trendlines follow similar upward trajectories that broadly track GDP growth

<br>

* The Producer Price Index measures the average changes in prices received by domestic producers for their output
* Looking at the trendline for Producer Price Index, we can see that the cost of doing business for producers was relatively quite low until the 1980’s
  * This upward shift can largely be attributed to the anti-inflationary, high interest rate       policies of former Fed Chairman, Paul Vlocker 
* Since the 1980’s the PPI increases at a rate similar to that of Industrial Production


```{r, dpi=300}
#plotting Industrial production and PPI
colors <- c("PPI:Durable goods" = "blue", "Industrial Production" = "red")
a1<- ggplot(NULL, aes(Month,log(Index))) +
  geom_line(data = industrial_prod_durable_goods, aes(col = "Industrial Production"))+
  geom_line(data = PPI_durable_goods, aes(col = "PPI:Durable goods"))+
  labs(x = "Month", y = "log(Index)" ,color = "PPI/Industrial Production")+
  ggtitle("PPI by commodity(Final Demand) and Industrial production : Durable Goods")+
  scale_color_manual(values = colors)

#plotting PPI and CPI
colors <- c("PPI:Durable goods" = "blue", "CPI:Durable" = "red")
a2<- ggplot(NULL, aes(Month,log(Index))) +
  geom_line(data = CPI_durable_goods, aes(col = "CPI:Durable"))+
  geom_line(data = PPI_durable_goods, aes(col = "PPI:Durable goods"))+
  labs(x = "Month", y = "log(Index)" ,color = "PPI/CPI:Durable")+
  ggtitle("PPI by commodity(Final Demand) and CPI : Durable Goods")+
  scale_color_manual(values = colors)
grid.arrange(a1,a2,nrow=2)
```


* Overlaying the CPI and PPI for Durable Goods, we can see that the two trendlines track one another closely until the early 2000’s, when they start to diverge with PPI increasing at higher rates than CPI 
  * This suggests that durable goods were becoming relatively cheaper for consumers to purchase     than for companies to produce
  * However, in 2020 with COVID related inflation, the CPI increased at a much higher rate         compared to PPI - narrowing a gap that had been widening for two decades
  
* Recent inflation readings show substantial increases in the producer price index for durable goods with the 12-month growth rate reaching 5.6% in April. We expect this to have a 12-month growth rate reaching 8.4% in December
* April figures for the consumer price index (CPI) are even higher, with 12-month inflation at 13.5% for durable goods. This high inflation is attributable to the strong demand for goods spurred by fiscal pandemic support combined with continued wage gains, global supply chain disruptions, and the slow labor supply recovery because of ongoing COVID-19 concerns. We expect inflation to moderate as these factors resolve themselves and monetary policy tightening impacts the economy


```{r, dpi=300}
#Since best model for industrial production was auto arima model when we compared different models, so using it

ip_model <- industrial_prod_durable_goods %>%
  model(MAdM = ETS(log(Index) ~ error("M") +  trend("Ad") + season("M")))
forecast2 <- ip_model %>% forecast(h = "1 years")
f3<- forecast2 %>% autoplot(industrial_prod_durable_goods %>% filter(year(Month)>2000)) +
  labs(y = "Index", title = "Indusrtial Production: Durable Goods")

#Since best model for PPI was auto arima model when we compared different models, so using it

ppi_model <- PPI_durable_goods %>%
  model(arima_model = ARIMA(log(Index), stepwise = FALSE, approx = FALSE))
forecast3 <- ppi_model %>% forecast(h = "1 years")
f4<- forecast3 %>% autoplot(PPI_durable_goods%>% filter(year(Month)>2000)) +
  scale_y_continuous(trans='log10')+
  labs(y = "Index", title = "Producer Price Index by Commodity: Durable Goods")

#Since best model for CPI was ETS(A,Ad,N) model when we compared different models, so using it

cpi_model <- CPI_durable_goods %>%
  model(AAdN = ETS(log(Index) ~ error("A") + trend("Ad") + season("N")))
forecast4 <- cpi_model %>% forecast(h = "1 years")
f5<- forecast4 %>% autoplot(CPI_durable_goods %>% filter(year(Month)>2000)) +
  scale_y_continuous(trans='log10')+
  labs(y = "Index", title = "Consumer Price Index for all Urban Consumers: Durable Goods")
  

grid.arrange(f3,f4,f5,nrow=3)
```


<br>

**Real Personal Consumption Expenditure: Non-Durable Goods**

* The chart below shows what percentage of GDP is driven by (inflation-adjusted) personal consumption expenditures on Non-Durable Goods
* Compared to Durable Goods, we see much clearer seasonality in the data
  * This makes sense as Non-Durable Goods are generally cheaper than Durable Goods, and there      are certain times of the year (holiday season) when consumers will spend disproportionate      amounts relative to the rest of the year on Non-Durables
* We see that around the beginning of the pandemic there was an increase of about 2% in Consumer Expenditure on Non-Durable Goods 
* The forecast points to a seasonally-adjusted plateau of expenditures on Non-Durables


```{r, dpi=300}
#Since best model for rpce was ETS(MNM) when we compared different models, so using it
rpce_model_n <- rpce_ndurable_goods_gdp %>%
  model(AAA = ETS(Expenditure ~ error("M") +  trend("N") + season("M")))
forecast5 <- rpce_model_n %>% forecast(h = "1 years")
f5<- forecast5 %>% autoplot(rpce_ndurable_goods_gdp) +
  labs(y = "Expenditure(%of GDP)", title = "Real Personal Consumption Expenditure:Non-Durable Goods")
n1<- rpce_ndurable_goods_gdp %>%
  gg_season(Expenditure) + ggtitle("RPCE: Non-Durable Goods")+ylab("% of GDP")
grid.arrange(f5,n1,nrow=2)
```

<br>

**Industrial Production/PPI and PPI/CPI: Non-Durable Goods**

* In the chart below we see relatively stagnant growth in Industrial Production compared to a sharply increasing PPI trend line
  * These two facts are not unrelated - the cost of doing business and making Non-Durable goods     in the United States has increased dramatically of the past half century
  * Many jobs pertaining to Non-Durable Good production have been outsourced internationally       for cheaper labor and materials
  * In short, because of PPI rising Industrial Production has slowed


```{r, dpi=300}
#plotting Industrial production and PPI
colors <- c("PPI:Non-Durable goods" = "blue", "Industrial Production" = "red")
b1<- ggplot(NULL, aes(Month,log(Index))) +
  geom_line(data = industrial_prod_ndurable_goods, aes(col = "Industrial Production"))+
  geom_line(data = PPI_ndurable_goods, aes(col = "PPI:Non-Durable goods"))+
  labs(x = "Month", y = "log(Index)" ,color = "PPI/Industrial Production")+
  ggtitle("PPI by commodity(Final Demand) and Industrial production : Non-Durable Goods")+
  scale_color_manual(values = colors)

#plotting PPI and CPI
colors <- c("PPI:Non-Durable goods" = "blue", "CPI:NonDurable" = "red")
b2<- ggplot(NULL, aes(Month,log(Index))) +
  geom_line(data = CPI_ndurable_goods, aes(col = "CPI:NonDurable"))+
  geom_line(data = PPI_ndurable_goods, aes(col = "PPI:Non-Durable goods"))+
  labs(x = "Month", y = "log(Index)" ,color = "PPI/CPI:NonDurable")+
  ggtitle("PPI by commodity(Final Demand) and CPI :Non- Durable Goods")+
  scale_color_manual(values = colors)

grid.arrange(b1,b2,nrow =2)
```

* Because Non-Durable goods tend to be cheaper and more regularly purchased by Consumers, we see a much closer relationship between the PPI and CPI trend lines for Non-Durable Goods than for Durables
* In other words, with Non-Durable goods we see a clear pattern of Producers passing higher costs onto Consumers in short order
* The forecasts for CPI and PPI show them increasing at similar rates over the next one to two years

```{r, dpi=300}
#Since best model for industrial production was ETS(MAA) model when we compared different models, so using it

ip_model1 <- industrial_prod_ndurable_goods%>%
  model(MAdM = ETS(log(Index) ~ error("M") +  trend("A") + season("A")))
forecast6 <- ip_model1 %>% forecast(h = "1 years")
f6<- forecast6 %>% autoplot(industrial_prod_ndurable_goods %>% filter(year(Month)>2000)) +
  labs(y = "Index", title = "Indusrtial Production: Non Durable Goods")

#Since best model for PPI was auto arima model when we compared different models, so using it

ppi_model1 <- PPI_ndurable_goods %>%
  model(arima_model = ARIMA(log(Index), stepwise = FALSE, approx = FALSE))
forecast7 <- ppi_model1 %>% forecast(h = "1 years")
f7<- forecast7 %>% autoplot(PPI_ndurable_goods%>% filter(year(Month)>2000))+
  labs(y = "Index", title = "Producer Price Index by Commodity: Non Durable Goods")

#Since best model for CPI was arima auto model when we compared different models, so using it

cpi_model1 <- CPI_ndurable_goods %>%
  model(arima_model = ARIMA(log(Index), stepwise = FALSE, approx = FALSE))
forecast8 <- cpi_model1 %>% forecast(h = "1 years")
f8<- forecast8 %>% autoplot(CPI_ndurable_goods %>% filter(year(Month)>2000)) +
  labs(y = "Index", title = "Consumer Price Index for all Urban Consumers: Non Durable Goods")

grid.arrange(f6,f7,f8,nrow=3)

```


<br>

**Real Personal Consumption Expenditure: Services**

* Unsurprisingly, in 2020 when the pandemic and subsequent quarantine lockdowns took place, there was a sharp decrease in Service based Consumer Expenditures
* These numbers have yet to return to pre-pandemic levels, but the forecast shows a seasonally-adjusted upward trend the pre-pandemic level


```{r, dpi=300}
#Since best model for rpce was auto arima when we compared different models, so using it
rpce_model_s <- data_rpce_services_gdp %>%
  model(arima_model = ARIMA(Expenditure, stepwise = FALSE, approx = FALSE))
forecast9 <- rpce_model_s %>% forecast(h = "1 years")
f9<- forecast9 %>% autoplot(data_rpce_services_gdp) +
  labs(y = "Expenditure(%of GDP)", title = "Real Personal Consumption Expenditure:Services")
a1<- data_rpce_services_gdp %>% gg_season(Expenditure) + ggtitle("RPCE: Services")+ylab("% of GDP")
grid.arrange(f9,a1,nrow=2)

```

<br>
**Net Exports: Services**

* The below chart for Net Exports of Services as a percentage of GDP contains several insights
  * Exporting Services is becoming a smaller and smaller fraction of GDP 
  * The United States is importing more services at an increasing rate 
  * This may be attributed to the pool of skilled workers in the United States continuing to       grow, thus there is a lower domestic dependence on skilled workers from overseas


```{r, dpi=300}
#Since best model for net exports was auto arima when we compared different models, so using it
netexp_model <- data_netexp_gdp %>%
  model(arima_model = ARIMA(Expenditure, stepwise = FALSE, approx = FALSE))
forecast10 <- netexp_model %>% forecast(h = "1 years")
f10<- forecast10 %>% autoplot(data_netexp_gdp) +
  labs(y = "Expenditure(%of GDP)", title = "Net Exports:Services")

a2<- data_netexp_gdp %>% gg_season(Expenditure) + ggtitle("Net Exports: Services")

grid.arrange(f10,a2,nrow=2)
```

<br>

**All Employees-Service Providing**


* The number of Service Providing Employees serves as the supply of services in the United States, since there is no physical product being created
* We have seen a consist upward trend in the number of service providing employees over the past several decades, and the forecast shows this trend continuing  

```{r}
#Since best model  was auto arima when we compared different models, so using it
es_model <- data_ae_sp_1 %>%
  model(arima_model = ARIMA(SRVPRD, stepwise = FALSE, approx = FALSE))
forecast11 <- es_model %>% forecast(h = "1 years")
f11<- forecast11 %>% autoplot(data_ae_sp_1) +
  labs(y = "Amount", title = "All Employees-Service Providing")

a3<- data_ae_sp_1 %>% gg_season(SRVPRD) + ggtitle("All Employees-Service Providing")
grid.arrange(f11,a3,nrow=2)
```


<br>

**References**

* “Federal Reserve Economic Data: Fred: St. Louis Fed.” FRED, Federal Reserve Bank of St.            Louis, https://fred.stlouisfed.org/
* Federal Reserve Bank of San Francisco. “Economic Research.” Federal Reserve Bank of San            Francisco, Federal Reserve Bank of San Francisco, 21 Dec. 2020,                                https://www.frbsf.org/economic-research/ 
* Meet the Authors Kristen Tauber Research Analyst Kristen Tauber is a research analyst in the       Research Department of the Federal Reserve . “Why Has Durable Goods Spending Been so           Strong during the COVID-19 Pandemic?” Website, Federal Reserve Bank of Cleveland, 7 July       2021, https://www.clevelandfed.org/en/newsroom-and-events/publications/economic-commentar       y/2021-economic-commentaries/ec-202116-durable-goods-spending-during-covid19-pandemic.asp